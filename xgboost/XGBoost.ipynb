{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "113961b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TARGET scale] train: {'min': 0.0, 'q50': 0.0, 'max': 55866.048}\n",
      "[TARGET scale] test : {'min': 0.0, 'q50': 0.0, 'max': 5.3754}\n",
      "🔧 입력/타깃 단위 보정 배수: raw≈5527.50, chosen=10000\n",
      "ℹ️ GPU 미사용: Invalid Input: 'gpu_hist', valid values are: {'approx', 'auto', 'exact', 'hist'}\n",
      "➡️ CPU 사용: tree_method=hist, predictor=auto\n",
      "▶ 모델 학습 중... (라운드=100, 검증 구간=7일)\n",
      "[Validation] MAE=197.6640 | RMSE=1066.4473 | R2=0.9037\n",
      "✅ 예측 결과 저장: C:\\ESG_Project1\\file\\merge_data\\xgb_week2day_predictions.csv\n",
      "✅ 모델 파이프라인 저장: C:\\ESG_Project1\\file\\merge_data\\xgb_week2day_pipeline.pkl\n",
      "[Test] MAE=365.3847 | RMSE=1931.6434 | R2=0.8466\n",
      "🧪 디버그 샘플 저장: C:\\ESG_Project1\\file\\merge_data\\xgb_week2day_debug_sample.csv\n",
      "ℹ️ 메타 저장 완료\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# XGBoost 주간학습 다음날 예측 (100 에포크, 조용한 출력)\n",
    "# - GPU 가능시 GPU, 불가시 자동 CPU 폴백\n",
    "# - train_data2.csv, test_data2.csv 사용\n",
    "# - 타깃: \"합산발전량(MWh)\" (회귀)\n",
    "# - 출력: 예측 CSV, 모델 파이프라인(pkl), 메타 JSON\n",
    "# - 포함: 스케일 점검, 테스트 피처/타깃 단위 보정(강제), 정렬 일치 평가\n",
    "# ==========================================\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# ---------------------------\n",
    "# 0. 사용자 설정\n",
    "# ---------------------------\n",
    "TRAIN_PATH = r\"C:\\ESG_Project1\\file\\merge_data\\train_data.csv\"\n",
    "TEST_PATH  = r\"C:\\ESG_Project1\\file\\merge_data\\test_data.csv\"\n",
    "TARGET_COL = \"합산발전량(MWh)\"\n",
    "GROUP_KEYS = [\"발전구분\", \"지역\", \"지점번호\"]\n",
    "\n",
    "OUT_PREFIX = \"xgb_week2day\"\n",
    "RANDOM_STATE = 42\n",
    "EPOCHS = 100\n",
    "VAL_DAYS = 7\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "LAG_MAX = 168\n",
    "ROLL_WINDOWS = [24, 72, 168]\n",
    "\n",
    "# ---------------------------\n",
    "# 1. 유틸\n",
    "# ---------------------------\n",
    "def read_csv_flex(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=\"utf-8\", low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"cp949\", low_memory=False)\n",
    "\n",
    "def detect_datetime(df: pd.DataFrame):\n",
    "    for c in [\"일시\", \"datetime\", \"timestamp\", \"DATE_TIME\", \"date_time\"]:\n",
    "        if c in df.columns:\n",
    "            dt = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "            if dt.notna().any():\n",
    "                df[\"__dt__\"] = dt\n",
    "                return \"__dt__\"\n",
    "    date_cands = [\"일자\", \"date\"]\n",
    "    hour_cands = [\"시간\", \"hour\", \"HOUR\"]\n",
    "    date_col = next((c for c in date_cands if c in df.columns), None)\n",
    "    hour_col = next((c for c in hour_cands if c in df.columns), None)\n",
    "    if date_col and hour_col:\n",
    "        h = df[hour_col].astype(str).str.extract(r\"(\\d{1,2})\")[0].astype(float).clip(0, 23)\n",
    "        df[\"__dt__\"] = pd.to_datetime(df[date_col], errors=\"coerce\") + pd.to_timedelta(h, unit=\"h\")\n",
    "        return \"__dt__\"\n",
    "    return None\n",
    "\n",
    "def add_time_features(df: pd.DataFrame, dt_col: str):\n",
    "    df[\"hour\"] = df[dt_col].dt.hour\n",
    "    df[\"dayofweek\"] = df[dt_col].dt.dayofweek\n",
    "    df[\"month\"] = df[dt_col].dt.month\n",
    "    df[\"is_weekend\"] = (df[\"dayofweek\"] >= 5).astype(int)\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24.0)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24.0)\n",
    "    df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12.0)\n",
    "    df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12.0)\n",
    "    return df\n",
    "\n",
    "def ensure_keys(df, keys):\n",
    "    return [k for k in (keys or []) if k in df.columns]\n",
    "\n",
    "def add_lags_and_rolls(df_all: pd.DataFrame, dt_col: str, keys, target: str):\n",
    "    df_all = df_all.sort_values(ensure_keys(df_all, keys) + [dt_col] if keys else [dt_col]).copy()\n",
    "    # 래그\n",
    "    lag_blocks = []\n",
    "    for lag in range(1, LAG_MAX + 1):\n",
    "        series = (df_all.groupby(keys)[target].shift(lag) if keys\n",
    "                  else df_all[target].shift(lag))\n",
    "        lag_blocks.append(series.rename(f\"lag_{lag}\"))\n",
    "    lag_df = pd.concat(lag_blocks, axis=1)\n",
    "    # 롤링 (정보누수 방지: shift(1) 후 rolling)\n",
    "    roll_mean_blocks, roll_std_blocks = [], []\n",
    "    for w in ROLL_WINDOWS:\n",
    "        if keys:\n",
    "            g = df_all.groupby(keys)[target].shift(1)\n",
    "            rm = g.rolling(w, min_periods=int(w*0.6)).mean()\n",
    "            rs = g.rolling(w, min_periods=int(w*0.6)).std()\n",
    "        else:\n",
    "            rm = df_all[target].shift(1).rolling(w, min_periods=int(w*0.6)).mean()\n",
    "            rs = df_all[target].shift(1).rolling(w, min_periods=int(w*0.6)).std()\n",
    "        roll_mean_blocks.append(rm.rename(f\"roll_mean_{w}\"))\n",
    "        roll_std_blocks.append(rs.rename(f\"roll_std_{w}\"))\n",
    "    roll_df = pd.concat(roll_mean_blocks + roll_std_blocks, axis=1)\n",
    "    return pd.concat([df_all, lag_df, roll_df], axis=1)\n",
    "\n",
    "def make_onehot_encoder():\n",
    "    # scikit-learn 버전 차이 호환\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "def choose_tree_method_flexible():\n",
    "    \"\"\"GPU 가능하면 gpu_hist, 아니면 hist로 폴백.\"\"\"\n",
    "    try:\n",
    "        probe = XGBRegressor(\n",
    "            n_estimators=1,\n",
    "            max_depth=1,\n",
    "            tree_method=\"gpu_hist\",\n",
    "            predictor=\"gpu_predictor\",\n",
    "            verbosity=0,\n",
    "        )\n",
    "        probe.fit(np.array([[0.0, 0.0]]), np.array([0.0]))\n",
    "        booster = probe.get_booster()\n",
    "        pred_attr = booster.attributes().get(\"predictor\", \"\")\n",
    "        if \"gpu\" in pred_attr:\n",
    "            print(\"✅ GPU 사용: tree_method=gpu_hist, predictor=gpu_predictor\")\n",
    "            return \"gpu_hist\", \"gpu_predictor\", True\n",
    "    except Exception as e:\n",
    "        print(f\"ℹ️ GPU 미사용: {e}\")\n",
    "    print(\"➡️ CPU 사용: tree_method=hist, predictor=auto\")\n",
    "    return \"hist\", \"auto\", False\n",
    "\n",
    "def _q(s):\n",
    "    s = np.asarray(s, dtype=float); s = s[np.isfinite(s)]\n",
    "    if s.size == 0: return {\"min\": None, \"q50\": None, \"max\": None}\n",
    "    return dict(min=float(np.nanmin(s)), q50=float(np.nanmedian(s)), max=float(np.nanmax(s)))\n",
    "\n",
    "def _p(a, q):\n",
    "    a = np.asarray(a, dtype=float); a = a[np.isfinite(a)]\n",
    "    return np.percentile(a, q) if a.size else np.nan\n",
    "\n",
    "# ---------------------------\n",
    "# 2. 데이터 로드\n",
    "# ---------------------------\n",
    "train = read_csv_flex(TRAIN_PATH)\n",
    "test  = read_csv_flex(TEST_PATH)\n",
    "\n",
    "dt_col_train = detect_datetime(train)\n",
    "dt_col_test  = detect_datetime(test)\n",
    "if dt_col_train is None or dt_col_test is None:\n",
    "    raise ValueError(\"datetime 컬럼을 찾을 수 없습니다. (일시 또는 일자+시간 필요)\")\n",
    "\n",
    "train = add_time_features(train, dt_col_train)\n",
    "test  = add_time_features(test, dt_col_test)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. 히스토리 결합 후 래그/롤링 생성\n",
    "# ---------------------------\n",
    "all_df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "all_df = add_lags_and_rolls(all_df, \"__dt__\", GROUP_KEYS, TARGET_COL)\n",
    "\n",
    "# 분리\n",
    "n_train = len(train)\n",
    "train_df = all_df.iloc[:n_train].copy()\n",
    "test_df  = all_df.iloc[n_train:].copy()\n",
    "\n",
    "# ---------------------------\n",
    "# 4. 테스트 피처/타깃 단위 보정 (강제 버전)\n",
    "# ---------------------------\n",
    "lag_cols  = [f\"lag_{i}\" for i in range(1, LAG_MAX + 1)]\n",
    "roll_cols = [c for c in test_df.columns if c.startswith(\"roll_mean_\") or c.startswith(\"roll_std_\")]\n",
    "cands = np.array([1, 10, 24, 100, 1000, 10000])\n",
    "\n",
    "tr_p95 = _p(train_df[TARGET_COL].values, 95); tr_max = _p(train_df[TARGET_COL].values, 100)\n",
    "te_p95_lag1 = _p(test_df.get(\"lag_1\", pd.Series(np.nan, index=test_df.index)).values, 95)\n",
    "te_max_lag1 = _p(test_df.get(\"lag_1\", pd.Series(np.nan, index=test_df.index)).values, 100)\n",
    "\n",
    "ratios = []\n",
    "if np.isfinite(tr_p95) and np.isfinite(te_p95_lag1) and te_p95_lag1 > 0:\n",
    "    ratios.append(tr_p95 / te_p95_lag1)\n",
    "if np.isfinite(tr_max) and np.isfinite(te_max_lag1) and te_max_lag1 > 0:\n",
    "    ratios.append(tr_max / te_max_lag1)\n",
    "\n",
    "te_max_target = _p(test_df[TARGET_COL].values, 100)\n",
    "if (not ratios) and np.isfinite(tr_max) and np.isfinite(te_max_target) and te_max_target > 0:\n",
    "    ratios.append(tr_max / te_max_target)\n",
    "\n",
    "raw_factor = float(np.median(ratios)) if ratios else 1.0\n",
    "unit_factor = float(cands[np.argmin(np.abs(cands - raw_factor))])\n",
    "if not np.isfinite(unit_factor) or unit_factor <= 0: unit_factor = 1.0\n",
    "\n",
    "print(\"[TARGET scale] train:\", _q(train_df[TARGET_COL].values))\n",
    "print(\"[TARGET scale] test :\", _q(test_df[TARGET_COL].values))\n",
    "print(f\"🔧 입력/타깃 단위 보정 배수: raw≈{raw_factor:.2f}, chosen={unit_factor:.0f}\")\n",
    "\n",
    "if unit_factor != 1.0:\n",
    "    scale_cols = [c for c in lag_cols + roll_cols if c in test_df.columns]\n",
    "    test_df.loc[:, scale_cols] = test_df.loc[:, scale_cols].astype(float) * unit_factor\n",
    "\n",
    "# ---------------------------\n",
    "# 5. 학습/검증 분리\n",
    "# ---------------------------\n",
    "valid_mask_train = train_df[lag_cols].notna().all(axis=1)\n",
    "X_train = train_df.loc[valid_mask_train].drop(columns=[TARGET_COL, \"__dt__\"], errors=\"ignore\")\n",
    "y_train = train_df.loc[valid_mask_train, TARGET_COL].astype(float)\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET_COL, \"__dt__\"], errors=\"ignore\")\n",
    "y_test = test_df[TARGET_COL] if TARGET_COL in test_df.columns else None\n",
    "\n",
    "cutoff = train_df[\"__dt__\"].max() - pd.Timedelta(days=VAL_DAYS)\n",
    "val_mask = train_df.loc[valid_mask_train, \"__dt__\"] >= cutoff\n",
    "X_tr, X_val = X_train.loc[~val_mask], X_train.loc[val_mask]\n",
    "y_tr, y_val = y_train.loc[~val_mask], y_train.loc[val_mask]\n",
    "\n",
    "# ---------------------------\n",
    "# 6. 전처리/모델 구성 및 학습 (GPU 가능시 GPU)\n",
    "# ---------------------------\n",
    "tree_method, predictor, gpu_used = choose_tree_method_flexible()\n",
    "\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "num_tf = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "cat_tf = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", make_onehot_encoder()),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[(\"num\", num_tf, numeric_cols), (\"cat\", cat_tf, categorical_cols)],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3,\n",
    ")\n",
    "\n",
    "preprocess.fit(X_train)\n",
    "X_tr_prep  = preprocess.transform(X_tr)\n",
    "X_val_prep = preprocess.transform(X_val)\n",
    "\n",
    "model = XGBRegressor(\n",
    "    n_estimators=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    max_depth=8,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    tree_method=tree_method,   # 'gpu_hist' or 'hist'\n",
    "    predictor=predictor,       # 'gpu_predictor' or 'auto'\n",
    "    eval_metric=\"rmse\",\n",
    "    verbosity=0,\n",
    ")\n",
    "\n",
    "print(f\"▶ 모델 학습 중... (라운드={EPOCHS}, 검증 구간={VAL_DAYS}일)\")\n",
    "model.fit(X_tr_prep, y_tr)\n",
    "\n",
    "val_pred = model.predict(X_val_prep)\n",
    "val_mae  = mean_absolute_error(y_val, val_pred)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "val_r2   = r2_score(y_val, val_pred)\n",
    "print(f\"[Validation] MAE={val_mae:.4f} | RMSE={val_rmse:.4f} | R2={val_r2:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. 예측/저장\n",
    "# ---------------------------\n",
    "X_test_prep = preprocess.transform(X_test)\n",
    "test_pred = model.predict(X_test_prep)\n",
    "\n",
    "out_dir = os.path.dirname(TEST_PATH)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "pred_df = test.copy()\n",
    "pred_df[\"예측합산발전량(MWh)\"] = test_pred\n",
    "pred_path = os.path.join(out_dir, f\"{OUT_PREFIX}_predictions.csv\")\n",
    "pred_df.to_csv(pred_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ 예측 결과 저장: {pred_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 8. 파이프라인/메타 저장 + 평가(정렬 일치, 단위 일치)\n",
    "# ---------------------------\n",
    "pipe = Pipeline([(\"preprocess\", preprocess), (\"model\", model)])\n",
    "model_path = os.path.join(out_dir, f\"{OUT_PREFIX}_pipeline.pkl\")\n",
    "joblib.dump(pipe, model_path)\n",
    "print(f\"✅ 모델 파이프라인 저장: {model_path}\")\n",
    "\n",
    "metrics = None\n",
    "if y_test is not None and y_test.notna().any():\n",
    "    eval_df = test_df[[\"__dt__\", *ensure_keys(test_df, GROUP_KEYS), TARGET_COL]].copy()\n",
    "    eval_df[\"y_true\"] = eval_df[TARGET_COL].astype(float) * unit_factor  # 타깃만 단위 보정\n",
    "    eval_df[\"y_pred\"] = test_pred                                        # 예측값은 피처 보정 반영\n",
    "\n",
    "    valid_lags_mask_test = test_df[lag_cols].notna().all(axis=1)\n",
    "    mask_eval = np.isfinite(eval_df[\"y_true\"]) & valid_lags_mask_test\n",
    "    eval_df = eval_df[mask_eval].copy()\n",
    "\n",
    "    mae  = mean_absolute_error(eval_df[\"y_true\"], eval_df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(eval_df[\"y_true\"], eval_df[\"y_pred\"]))\n",
    "    r2   = r2_score(eval_df[\"y_true\"], eval_df[\"y_pred\"])\n",
    "    metrics = {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2),\n",
    "               \"unit_factor\": float(unit_factor), \"gpu_used\": bool(gpu_used)}\n",
    "    print(f\"[Test] MAE={mae:.4f} | RMSE={rmse:.4f} | R2={r2:.4f}\")\n",
    "    dbg_path = os.path.join(out_dir, f\"{OUT_PREFIX}_debug_sample.csv\")\n",
    "    eval_df[[\"__dt__\", \"y_true\", \"y_pred\"]].head(10).to_csv(dbg_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"🧪 디버그 샘플 저장: {dbg_path}\")\n",
    "else:\n",
    "    print(\"ℹ️ 테스트 타깃이 없어 테스트 지표를 계산하지 않았습니다.\")\n",
    "\n",
    "meta = {\n",
    "    \"train_path\": TRAIN_PATH,\n",
    "    \"test_path\": TEST_PATH,\n",
    "    \"target\": TARGET_COL,\n",
    "    \"datetime_col\": \"__dt__\",\n",
    "    \"group_keys_used\": ensure_keys(train, GROUP_KEYS),\n",
    "    \"tree_method\": tree_method,\n",
    "    \"predictor\": predictor,\n",
    "    \"gpu_used\": bool(gpu_used),\n",
    "    \"lags\": LAG_MAX,\n",
    "    \"roll_windows\": ROLL_WINDOWS,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"val_days\": VAL_DAYS,\n",
    "    \"numeric_cols\": X_train.select_dtypes(include=[np.number]).columns.tolist(),\n",
    "    \"categorical_cols\": X_train.select_dtypes(exclude=[np.number]).columns.tolist(),\n",
    "    \"metrics_val\": {\"MAE\": float(val_mae), \"RMSE\": float(val_rmse), \"R2\": float(val_r2)},\n",
    "    \"metrics_test\": metrics,\n",
    "}\n",
    "with open(os.path.join(out_dir, f\"{OUT_PREFIX}_meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"ℹ️ 메타 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "edb71537",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\ESG_Project1\\\\file\\\\merge_data\\\\test_data2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     80\u001b[39m UNIT_FACTOR  = \u001b[38;5;28mfloat\u001b[39m(meta.get(\u001b[33m\"\u001b[39m\u001b[33mmetrics_test\u001b[39m\u001b[33m\"\u001b[39m, {}).get(\u001b[33m\"\u001b[39m\u001b[33munit_factor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1.0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1.0\u001b[39m)\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# ---------- 평가 데이터 ----------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m eval_df = \u001b[43mread_csv_flex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSV_TO_EVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m dt_col = detect_datetime(eval_df)\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dt_col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdatetime 컬럼을 찾을 수 없습니다.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mread_csv_flex\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_csv_flex\u001b[39m(p):\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m pd.read_csv(p, encoding=\u001b[33m\"\u001b[39m\u001b[33mcp949\u001b[39m\u001b[33m\"\u001b[39m, low_memory=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ESG_Project1\\.venv310\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ESG_Project1\\.venv310\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ESG_Project1\\.venv310\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ESG_Project1\\.venv310\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ESG_Project1\\.venv310\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\ESG_Project1\\\\file\\\\merge_data\\\\test_data2.csv'"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 이상치 탐지 & 시각화 (Top p%)\n",
    "# - unit_factor: train tail만 보정(/=)  ← ★중요\n",
    "# - y_true/y_pred에는 추가 곱셈 없음(중복 방지)\n",
    "# - 2024년 이후만 분석/시각화\n",
    "# ==========================================\n",
    "import os, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# ---------- 설정 ----------\n",
    "CSV_TO_EVAL = r\"C:\\ESG_Project1\\file\\merge_data\\test_data2.csv\"\n",
    "OUT_PREFIX  = \"xgb_week2day\"\n",
    "WORK_DIR    = os.path.dirname(CSV_TO_EVAL)\n",
    "MODEL_PATH  = os.path.join(WORK_DIR, f\"{OUT_PREFIX}_pipeline.pkl\")\n",
    "META_PATH   = os.path.join(WORK_DIR, f\"{OUT_PREFIX}_meta.json\")\n",
    "\n",
    "TOP_P = 0.01\n",
    "WEEK_START = \"2024-01-01\"\n",
    "\n",
    "def _set_korean_font():\n",
    "    try:\n",
    "        if os.name == \"nt\": matplotlib.rc(\"font\", family=\"Malgun Gothic\")\n",
    "        elif hasattr(os, \"uname\") and os.uname().sysname == \"Darwin\":\n",
    "            matplotlib.rc(\"font\", family=\"AppleGothic\")\n",
    "        matplotlib.rcParams[\"axes.unicode_minus\"] = False\n",
    "    except: pass\n",
    "_set_korean_font()\n",
    "\n",
    "def read_csv_flex(p):\n",
    "    try: return pd.read_csv(p, encoding=\"utf-8\", low_memory=False)\n",
    "    except UnicodeDecodeError: return pd.read_csv(p, encoding=\"cp949\", low_memory=False)\n",
    "\n",
    "def detect_datetime(df):\n",
    "    for c in [\"일시\",\"datetime\",\"timestamp\",\"DATE_TIME\",\"date_time\",\"__dt__\"]:\n",
    "        if c in df.columns:\n",
    "            dt = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "            if dt.notna().any():\n",
    "                df[\"__dt__\"] = dt\n",
    "                return \"__dt__\"\n",
    "    return None\n",
    "\n",
    "def add_time_features(df, dt_col):\n",
    "    df[\"hour\"] = df[dt_col].dt.hour\n",
    "    df[\"dayofweek\"] = df[dt_col].dt.dayofweek\n",
    "    df[\"month\"] = df[dt_col].dt.month\n",
    "    df[\"is_weekend\"] = (df[\"dayofweek\"] >= 5).astype(int)\n",
    "    df[\"hour_sin\"] = np.sin(2*np.pi*df[\"hour\"]/24)\n",
    "    df[\"hour_cos\"] = np.cos(2*np.pi*df[\"hour\"]/24)\n",
    "    df[\"month_sin\"] = np.sin(2*np.pi*df[\"month\"]/12)\n",
    "    df[\"month_cos\"] = np.cos(2*np.pi*df[\"month\"]/12)\n",
    "    return df\n",
    "\n",
    "def ensure_keys(df, keys): \n",
    "    return [k for k in (keys or []) if k in df.columns]\n",
    "\n",
    "def add_lags_and_rolls(df_all, dt_col, keys, target, lag_max, roll_windows):\n",
    "    keys = ensure_keys(df_all, keys)\n",
    "    df_all = df_all.sort_values(keys + [dt_col] if keys else [dt_col]).copy()\n",
    "    g = df_all.groupby(keys) if keys else df_all.assign(_=1).groupby(\"_\")\n",
    "    for lag in range(1, lag_max+1):\n",
    "        df_all[f\"lag_{lag}\"] = g[target].shift(lag)\n",
    "    for w in roll_windows:\n",
    "        df_all[f\"roll_mean_{w}\"] = g[target].shift(1).rolling(w, min_periods=int(w*0.6)).mean()\n",
    "        df_all[f\"roll_std_{w}\"]  = g[target].shift(1).rolling(w, min_periods=int(w*0.6)).std()\n",
    "    return df_all\n",
    "\n",
    "# ---------- 메타 로드 ----------\n",
    "with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "TARGET_COL   = meta.get(\"target\", \"합산발전량(MWh)\")\n",
    "GROUP_KEYS   = meta.get(\"group_keys_used\", [])\n",
    "LAG_MAX      = int(meta.get(\"lags\", 168))\n",
    "ROLL_WINDOWS = list(meta.get(\"roll_windows\", [24,72,168]))\n",
    "TRAIN_PATH   = meta.get(\"train_path\")\n",
    "# unit_factor 없으면 1로 (추정 필요시 10000 말고 1로 두는게 안전)\n",
    "UNIT_FACTOR  = float(meta.get(\"metrics_test\", {}).get(\"unit_factor\", 1.0) or 1.0)\n",
    "\n",
    "# ---------- 평가 데이터 ----------\n",
    "eval_df = read_csv_flex(CSV_TO_EVAL)\n",
    "dt_col = detect_datetime(eval_df)\n",
    "if dt_col is None: raise ValueError(\"datetime 컬럼을 찾을 수 없습니다.\")\n",
    "eval_df = add_time_features(eval_df, dt_col)\n",
    "\n",
    "# ---------- train tail 이어붙이기 (★train tail만 /= unit_factor) ----------\n",
    "if TRAIN_PATH and os.path.exists(TRAIN_PATH):\n",
    "    tr = read_csv_flex(TRAIN_PATH)\n",
    "    dt_tr = detect_datetime(tr)\n",
    "    if dt_tr is None: raise ValueError(\"train_data에서 datetime 생성 실패\")\n",
    "    tr = add_time_features(tr, dt_tr)\n",
    "    if TARGET_COL in tr.columns:\n",
    "        # 학습(max≈55866), 테스트(max≈5.x)였다면 unit_factor≈10000\n",
    "        # → test 스케일에 맞추려면 train tail을 나눠서 작게 만들어야 함.\n",
    "        tr[TARGET_COL] = tr[TARGET_COL].astype(float) / UNIT_FACTOR\n",
    "    keys = ensure_keys(eval_df, GROUP_KEYS)\n",
    "    tail = tr.sort_values(keys + [dt_tr]).groupby(keys, as_index=False).tail(LAG_MAX) if keys else tr.sort_values(dt_tr).tail(LAG_MAX)\n",
    "    common = list(set(tail.columns) & set(eval_df.columns))\n",
    "    base = pd.concat([tail[common], eval_df[common]], ignore_index=True)\n",
    "else:\n",
    "    base = eval_df.copy()\n",
    "\n",
    "# 동일 방식으로 lag/roll 생성\n",
    "base = add_lags_and_rolls(base, \"__dt__\", GROUP_KEYS, TARGET_COL, LAG_MAX, ROLL_WINDOWS)\n",
    "eval_part = base.iloc[len(base) - len(eval_df):].copy()\n",
    "\n",
    "# ---------- 예측 (y_true/y_pred는 추가 스케일링 없음) ----------\n",
    "pipe = joblib.load(MODEL_PATH)\n",
    "X = eval_part.drop(columns=[c for c in [TARGET_COL, \"__dt__\"] if c in eval_part.columns], errors=\"ignore\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    y_pred = pipe.predict(X)\n",
    "\n",
    "pred = eval_part[[dt_col] + [k for k in ensure_keys(eval_part, GROUP_KEYS)]].copy()\n",
    "pred[\"y_pred\"] = y_pred\n",
    "pred[\"y_true\"] = eval_part[TARGET_COL].astype(float).values\n",
    "\n",
    "# ---------- 2024년 이후만 집계 ----------\n",
    "agg_full = pred.groupby(dt_col, as_index=False).sum(numeric_only=True).sort_values(dt_col)\n",
    "agg = agg_full[agg_full[dt_col] >= pd.Timestamp(\"2024-01-01\")].copy()\n",
    "\n",
    "# ---------- 지표/이상치 ----------\n",
    "agg[\"resid\"] = agg[\"y_true\"] - agg[\"y_pred\"]\n",
    "mae  = mean_absolute_error(agg[\"y_true\"], agg[\"y_pred\"])\n",
    "rmse = np.sqrt(mean_squared_error(agg[\"y_true\"], agg[\"y_pred\"]))\n",
    "r2   = r2_score(agg[\"y_true\"], agg[\"y_pred\"])\n",
    "thr  = np.quantile(np.abs(agg[\"resid\"].values), 1 - TOP_P)\n",
    "agg[\"is_outlier\"] = np.abs(agg[\"resid\"]) >= thr\n",
    "\n",
    "print(f\"전체 MAE {mae:.3f}  RMSE {rmse:.3f}  R² {r2:.3f}\")\n",
    "print(f\"[INFO] 상위 {int(TOP_P*100)}% 임계치: {thr:.3f}, 이상치 수: {int(agg['is_outlier'].sum())}\")\n",
    "\n",
    "# ---------- 시각화 ----------\n",
    "COLOR_TRUE = \"#1f77b4\"; COLOR_PRED = \"#ff7f0e\"; COLOR_OUTL = \"#d62728\"\n",
    "\n",
    "plt.figure(figsize=(18,5))\n",
    "plt.plot(agg[dt_col], agg[\"y_true\"], label=\"True\", linewidth=1.5, color=COLOR_TRUE)\n",
    "plt.plot(agg[dt_col], agg[\"y_pred\"], label=\"Pred\", linewidth=1.3, color=COLOR_PRED, alpha=0.9)\n",
    "out = agg[agg[\"is_outlier\"]]\n",
    "plt.scatter(out[dt_col], out[\"y_true\"], s=15, color=COLOR_OUTL, label=f\"Top {int(TOP_P*100)}% Outliers\")\n",
    "plt.title(f\"전체 결과 (Top {int(TOP_P*100)}% 이상치 강조, 2024~)\")\n",
    "plt.xlabel(\"Time\"); plt.ylabel(\"Value\"); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "start = pd.to_datetime(WEEK_START).normalize()\n",
    "end = start + pd.Timedelta(days=7)\n",
    "week = agg[(agg[dt_col] >= start) & (agg[dt_col] < end)]\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.plot(week[dt_col], week[\"y_true\"], label=\"True\", linewidth=1.5, color=COLOR_TRUE)\n",
    "plt.plot(week[dt_col], week[\"y_pred\"], label=\"Pred\", linewidth=1.3, color=COLOR_PRED, alpha=0.9)\n",
    "wk_out = week[week[\"is_outlier\"]]\n",
    "plt.scatter(wk_out[dt_col], wk_out[\"y_true\"], s=18, color=COLOR_OUTL, label=\"Outlier\")\n",
    "plt.title(f\"주간 확대: {start.date()} ~ {end.date()} (Top {int(TOP_P*100)}%)\")\n",
    "plt.xlabel(\"Time\"); plt.ylabel(\"Value\"); plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a84700d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
