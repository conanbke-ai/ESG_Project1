{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "113961b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ 모델 학습 중... (라운드=100, 검증 구간=7일)\n",
      "[Validation] MAE=20.6069 | RMSE=62.6544 | R2=0.9240\n",
      "✅ 예측 결과 저장: C:\\ESG_Project1\\file\\merge_data\\xgb_week2day_predictions.csv\n",
      "✅ 모델 파이프라인 저장: C:\\ESG_Project1\\file\\merge_data\\xgb_week2day_pipeline.pkl\n",
      "[Test] MAE=1.2314 | RMSE=11.3137 | R2=0.8937\n",
      "ℹ️ 메타 저장 완료\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# XGBoost 주간학습 다음날 예측 (100 에포크, 조용한 출력, 호환성 개선)\n",
    "# - train_data.csv, test_data.csv 사용\n",
    "# - 타깃: \"합산발전량(MWh)\" (회귀)\n",
    "# - 출력: 예측 CSV, 모델 파이프라인(pkl), 메타 JSON\n",
    "# ==========================================\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 전역: 불필요한 경고/로그 억제\n",
    "# -------------------------------------------------\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# ---------------------------\n",
    "# 0. 사용자 설정\n",
    "# ---------------------------\n",
    "TRAIN_PATH = r\"C:\\ESG_Project1\\file\\merge_data\\train_data.csv\"\n",
    "TEST_PATH  = r\"C:\\ESG_Project1\\file\\merge_data\\test_data.csv\"\n",
    "TARGET_COL = \"합산발전량(MWh)\"\n",
    "\n",
    "# 예: [\"발전구분\", \"지역\", \"지점번호\"]  (없으면 빈 리스트 [])\n",
    "GROUP_KEYS = [\"발전구분\", \"지역\", \"지점번호\"]\n",
    "\n",
    "OUT_PREFIX = \"xgb_week2day\"\n",
    "RANDOM_STATE = 42\n",
    "EPOCHS = 100               # ← 100 에포크 학습\n",
    "VAL_DAYS = 7               # ← 학습 데이터의 마지막 7일을 검증으로 사용\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "# 래그/롤링 설정\n",
    "LAG_MAX = 168              # 7일 * 24시간\n",
    "ROLL_WINDOWS = [24, 72, 168]\n",
    "\n",
    "# ---------------------------\n",
    "# 1. 유틸\n",
    "# ---------------------------\n",
    "def read_csv_flex(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=\"utf-8\", low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"cp949\", low_memory=False)\n",
    "\n",
    "def detect_datetime(df: pd.DataFrame):\n",
    "    # 단일 datetime 열\n",
    "    for c in [\"일시\", \"datetime\", \"timestamp\", \"DATE_TIME\", \"date_time\"]:\n",
    "        if c in df.columns:\n",
    "            dt = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "            if dt.notna().any():\n",
    "                df[\"__dt__\"] = dt\n",
    "                return \"__dt__\"\n",
    "    # 일자 + 시간\n",
    "    date_cands = [\"일자\", \"date\"]\n",
    "    hour_cands = [\"시간\", \"hour\", \"HOUR\"]\n",
    "    date_col = next((c for c in date_cands if c in df.columns), None)\n",
    "    hour_col = next((c for c in hour_cands if c in df.columns), None)\n",
    "    if date_col and hour_col:\n",
    "        h = df[hour_col].astype(str).str.extract(r\"(\\d{1,2})\")[0].astype(float).clip(0, 23)\n",
    "        df[\"__dt__\"] = pd.to_datetime(df[date_col], errors=\"coerce\") + pd.to_timedelta(h, unit=\"h\")\n",
    "        return \"__dt__\"\n",
    "    return None\n",
    "\n",
    "def add_time_features(df: pd.DataFrame, dt_col: str):\n",
    "    df[\"hour\"] = df[dt_col].dt.hour\n",
    "    df[\"dayofweek\"] = df[dt_col].dt.dayofweek\n",
    "    df[\"month\"] = df[dt_col].dt.month\n",
    "    df[\"is_weekend\"] = (df[\"dayofweek\"] >= 5).astype(int)\n",
    "    # 주기형 인코딩\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24.0)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24.0)\n",
    "    df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12.0)\n",
    "    df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12.0)\n",
    "    return df\n",
    "\n",
    "def ensure_keys(df, keys):\n",
    "    return [k for k in (keys or []) if k in df.columns]\n",
    "\n",
    "def add_lags_and_rolls(df_all: pd.DataFrame, dt_col: str, keys, target: str):\n",
    "    \"\"\"경고 최소화를 위해 래그/롤링을 한 번에 concat.\"\"\"\n",
    "    keys = ensure_keys(df_all, keys)\n",
    "    df_all = df_all.sort_values(keys + [dt_col] if keys else [dt_col]).copy()\n",
    "\n",
    "    # 래그 전체 생성\n",
    "    lag_blocks = []\n",
    "    for lag in range(1, LAG_MAX + 1):\n",
    "        series = (df_all.groupby(keys)[target].shift(lag) if keys\n",
    "                  else df_all[target].shift(lag))\n",
    "        lag_blocks.append(series.rename(f\"lag_{lag}\"))\n",
    "    lag_df = pd.concat(lag_blocks, axis=1)\n",
    "\n",
    "    # 롤링 평균/표준편차\n",
    "    roll_mean_blocks, roll_std_blocks = [], []\n",
    "    for w in ROLL_WINDOWS:\n",
    "        rm = (df_all.groupby(keys)[target].shift(1).rolling(w, min_periods=int(w*0.6)).mean()\n",
    "              if keys else df_all[target].shift(1).rolling(w).mean())\n",
    "        rs = (df_all.groupby(keys)[target].shift(1).rolling(w, min_periods=int(w*0.6)).std()\n",
    "              if keys else df_all[target].shift(1).rolling(w).std())\n",
    "        roll_mean_blocks.append(rm.rename(f\"roll_mean_{w}\"))\n",
    "        roll_std_blocks.append(rs.rename(f\"roll_std_{w}\"))\n",
    "    roll_df = pd.concat(roll_mean_blocks + roll_std_blocks, axis=1)\n",
    "\n",
    "    # 원본 + 파생 합치기\n",
    "    df_all = pd.concat([df_all, lag_df, roll_df], axis=1)\n",
    "    return df_all\n",
    "\n",
    "def choose_tree_method():\n",
    "    try:\n",
    "        XGBRegressor(n_estimators=1, tree_method=\"gpu_hist\", predictor=\"gpu_predictor\").fit(\n",
    "            np.array([[0, 0]]), np.array([0.0])\n",
    "        )\n",
    "        return \"gpu_hist\", \"gpu_predictor\"\n",
    "    except Exception:\n",
    "        return \"hist\", \"auto\"\n",
    "\n",
    "# ---------------------------\n",
    "# 2. 데이터 로드\n",
    "# ---------------------------\n",
    "train = read_csv_flex(TRAIN_PATH)\n",
    "test  = read_csv_flex(TEST_PATH)\n",
    "\n",
    "dt_col_train = detect_datetime(train)\n",
    "dt_col_test  = detect_datetime(test)\n",
    "if dt_col_train is None or dt_col_test is None:\n",
    "    raise ValueError(\"datetime 컬럼을 찾을 수 없습니다. (일시 또는 일자+시간 필요)\")\n",
    "\n",
    "# 시간 파생\n",
    "train = add_time_features(train, dt_col_train)\n",
    "test  = add_time_features(test, dt_col_test)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. 히스토리 결합 후 래그/롤링 생성\n",
    "# ---------------------------\n",
    "all_df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "all_df = add_lags_and_rolls(all_df, \"__dt__\", GROUP_KEYS, TARGET_COL)\n",
    "\n",
    "# 다시 분리\n",
    "n_train = len(train)\n",
    "train_df = all_df.iloc[:n_train].copy()\n",
    "test_df  = all_df.iloc[n_train:].copy()\n",
    "\n",
    "# 학습 입력/타깃\n",
    "lag_cols = [f\"lag_{i}\" for i in range(1, LAG_MAX + 1)]\n",
    "valid_mask = train_df[lag_cols].notna().all(axis=1)\n",
    "\n",
    "X_train = train_df.loc[valid_mask].drop(columns=[TARGET_COL, \"__dt__\"], errors=\"ignore\")\n",
    "y_train = train_df.loc[valid_mask, TARGET_COL].astype(float)\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET_COL, \"__dt__\"], errors=\"ignore\")\n",
    "y_test = test_df[TARGET_COL] if TARGET_COL in test_df.columns else None\n",
    "\n",
    "# ---------------------------\n",
    "# 4. 검증 분리 (학습 마지막 7일)\n",
    "# ---------------------------\n",
    "cutoff = train_df[\"__dt__\"].max() - pd.Timedelta(days=VAL_DAYS)\n",
    "val_mask = train_df.loc[valid_mask, \"__dt__\"] >= cutoff\n",
    "\n",
    "X_tr, X_val = X_train.loc[~val_mask], X_train.loc[val_mask]\n",
    "y_tr, y_val = y_train.loc[~val_mask], y_train.loc[val_mask]\n",
    "\n",
    "# ---------------------------\n",
    "# 5. 전처리/모델 구성 및 학습 (100 에포크, 조용)\n",
    "# ---------------------------\n",
    "tree_method, predictor = choose_tree_method()\n",
    "model = XGBRegressor(\n",
    "    n_estimators=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    max_depth=8,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    tree_method=tree_method,\n",
    "    predictor=predictor,\n",
    "    eval_metric=\"rmse\",\n",
    "    verbosity=0,          # 내부 로그 끔\n",
    ")\n",
    "\n",
    "# 전처리는 전체 학습셋 기준으로 fit → 데이터 일관성 보장\n",
    "num_tf = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "cat_tf = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)),\n",
    "])\n",
    "\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_tf, numeric_cols),\n",
    "        (\"cat\", cat_tf, categorical_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3,\n",
    ")\n",
    "\n",
    "# 전처리 학습(전체 학습셋) 후 변환\n",
    "preprocess.fit(X_train)\n",
    "X_tr_prep  = preprocess.transform(X_tr)\n",
    "X_val_prep = preprocess.transform(X_val)\n",
    "\n",
    "# 모델 학습(조용하게 100라운드)\n",
    "print(f\"▶ 모델 학습 중... (라운드={EPOCHS}, 검증 구간={VAL_DAYS}일)\")\n",
    "model.fit(X_tr_prep, y_tr)\n",
    "\n",
    "# 검증 점수(숫자 한 줄만)\n",
    "val_pred = model.predict(X_val_prep)\n",
    "val_mae  = mean_absolute_error(y_val, val_pred)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "val_r2   = r2_score(y_val, val_pred)\n",
    "print(f\"[Validation] MAE={val_mae:.4f} | RMSE={val_rmse:.4f} | R2={val_r2:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 6. 예측 및 파일 저장\n",
    "# ---------------------------\n",
    "X_test_prep = preprocess.transform(X_test)\n",
    "test_pred = model.predict(X_test_prep)\n",
    "\n",
    "out_dir = os.path.dirname(TEST_PATH)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "pred_df = test.copy()\n",
    "pred_df[\"예측합산발전량(MWh)\"] = test_pred\n",
    "pred_path = os.path.join(out_dir, f\"{OUT_PREFIX}_predictions.csv\")\n",
    "pred_df.to_csv(pred_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ 예측 결과 저장: {pred_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. 파이프라인/메타 저장\n",
    "# ---------------------------\n",
    "# 전처리(fitted) + 모델(fitted)을 파이프라인으로 묶어 저장\n",
    "pipe = Pipeline([(\"preprocess\", preprocess), (\"model\", model)])\n",
    "model_path = os.path.join(out_dir, f\"{OUT_PREFIX}_pipeline.pkl\")\n",
    "joblib.dump(pipe, model_path)\n",
    "print(f\"✅ 모델 파이프라인 저장: {model_path}\")\n",
    "\n",
    "# 테스트 성능(정답이 있을 때)\n",
    "metrics = None\n",
    "if y_test is not None and y_test.notna().any():\n",
    "    y_true = y_test.astype(float).values\n",
    "    mask_eval = np.isfinite(y_true)\n",
    "    if mask_eval.any():\n",
    "        mae = mean_absolute_error(y_true[mask_eval], test_pred[mask_eval])\n",
    "        rmse = np.sqrt(mean_squared_error(y_true[mask_eval], test_pred[mask_eval]))\n",
    "        r2 = r2_score(y_true[mask_eval], test_pred[mask_eval])\n",
    "        metrics = {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)}\n",
    "        print(f\"[Test] MAE={mae:.4f} | RMSE={rmse:.4f} | R2={r2:.4f}\")\n",
    "\n",
    "meta = {\n",
    "    \"train_path\": TRAIN_PATH,\n",
    "    \"test_path\": TEST_PATH,\n",
    "    \"target\": TARGET_COL,\n",
    "    \"datetime_col\": \"__dt__\",\n",
    "    \"group_keys_used\": ensure_keys(train, GROUP_KEYS),\n",
    "    \"tree_method\": tree_method,\n",
    "    \"predictor\": predictor,\n",
    "    \"lags\": LAG_MAX,\n",
    "    \"roll_windows\": ROLL_WINDOWS,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"val_days\": VAL_DAYS,\n",
    "    \"numeric_cols\": numeric_cols,\n",
    "    \"categorical_cols\": categorical_cols,\n",
    "    \"metrics_val\": {\"MAE\": float(val_mae), \"RMSE\": float(val_rmse), \"R2\": float(val_r2)},\n",
    "    \"metrics_test\": metrics,\n",
    "}\n",
    "with open(os.path.join(out_dir, f\"{OUT_PREFIX}_meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"ℹ️ 메타 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59312b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 이상치 시각화 (전체/주간 확대, Top p% 강조)\n",
    "# - 메타/모델/데이터 재활용\n",
    "# - 잔차 절대값(or anom_score) 상위 퍼센타일을 이상치로 표시\n",
    "# - 색상 고정: True=파랑, Pred=주황, Outlier=빨강\n",
    "# ==========================================\n",
    "import os, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- 사용자 설정 ----------\n",
    "CSV_TO_EVAL = r\"C:\\ESG_Project1\\file\\merge_data\\test_data.csv\"\n",
    "OUT_PREFIX  = \"xgb_week2day\"\n",
    "MODEL_PATH  = os.path.join(os.path.dirname(CSV_TO_EVAL), f\"{OUT_PREFIX}_pipeline.pkl\")\n",
    "META_PATH   = os.path.join(os.path.dirname(CSV_TO_EVAL), f\"{OUT_PREFIX}_meta.json\")\n",
    "\n",
    "# 특정 그룹만 보고 싶으면 지정 (예: {\"발전구분\":\"영동태양광\"})\n",
    "GROUP_FILTER = None\n",
    "\n",
    "# 이상치 상위 비율 (예: 0.01 = Top 1%)\n",
    "TOP_P = 0.01\n",
    "\n",
    "# 주간 확대: 시작 날짜(포함) 지정, None이면 전체 첫 날\n",
    "WEEK_START = None  # 예: \"2025-01-08\"\n",
    "\n",
    "# ---------- 한글 폰트 ----------\n",
    "def _set_korean_font():\n",
    "    try:\n",
    "        if os.name == \"nt\": matplotlib.rc(\"font\", family=\"Malgun Gothic\")\n",
    "        elif hasattr(os, \"uname\") and os.uname().sysname == \"Darwin\":\n",
    "            matplotlib.rc(\"font\", family=\"AppleGothic\")\n",
    "        matplotlib.rcParams[\"axes.unicode_minus\"] = False\n",
    "    except Exception:\n",
    "        pass\n",
    "_set_korean_font()\n",
    "\n",
    "# ---------- 유틸 ----------\n",
    "def read_csv_flex(p):\n",
    "    try:\n",
    "        return pd.read_csv(p, encoding=\"utf-8\", low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(p, encoding=\"cp949\", low_memory=False)\n",
    "\n",
    "def detect_datetime(df: pd.DataFrame):\n",
    "    for c in [\"일시\",\"datetime\",\"timestamp\",\"DATE_TIME\",\"date_time\",\"__dt__\"]:\n",
    "        if c in df.columns:\n",
    "            dt = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "            if dt.notna().any():\n",
    "                df[\"__dt__\"] = dt\n",
    "                return \"__dt__\"\n",
    "    dc = next((c for c in [\"일자\",\"date\"] if c in df.columns), None)\n",
    "    hc = next((c for c in [\"시간\",\"hour\",\"HOUR\"] if c in df.columns), None)\n",
    "    if dc and hc:\n",
    "        h = df[hc].astype(str).str.extract(r\"(\\d{1,2})\")[0].astype(float).clip(0,23).fillna(0).astype(int)\n",
    "        df[\"__dt__\"] = pd.to_datetime(df[dc], errors=\"coerce\") + pd.to_timedelta(h, unit=\"h\")\n",
    "        return \"__dt__\"\n",
    "    return None\n",
    "\n",
    "def add_time_features(df: pd.DataFrame, dt_col: str):\n",
    "    df[\"hour\"] = df[dt_col].dt.hour\n",
    "    df[\"dayofweek\"] = df[dt_col].dt.dayofweek\n",
    "    df[\"month\"] = df[dt_col].dt.month\n",
    "    df[\"is_weekend\"] = (df[\"dayofweek\"] >= 5).astype(int)\n",
    "    df[\"hour_sin\"] = np.sin(2*np.pi*df[\"hour\"]/24.0)\n",
    "    df[\"hour_cos\"] = np.cos(2*np.pi*df[\"hour\"]/24.0)\n",
    "    df[\"month_sin\"] = np.sin(2*np.pi*df[\"month\"]/12.0)\n",
    "    df[\"month_cos\"] = np.cos(2*np.pi*df[\"month\"]/12.0)\n",
    "    return df\n",
    "\n",
    "def ensure_keys(df, keys): \n",
    "    return [k for k in (keys or []) if k in df.columns]\n",
    "\n",
    "def add_lags_and_rolls(df_all: pd.DataFrame, dt_col: str, keys, target: str, lag_max: int, roll_windows):\n",
    "    keys = ensure_keys(df_all, keys)\n",
    "    df_all = df_all.sort_values(keys + [dt_col] if keys else [dt_col]).copy()\n",
    "    g = df_all.groupby(keys) if keys else df_all.assign(_=1).groupby(\"_\")\n",
    "    for lag in range(1, lag_max+1):\n",
    "        df_all[f\"lag_{lag}\"] = g[target].shift(lag)\n",
    "    for w in roll_windows:\n",
    "        df_all[f\"roll_mean_{w}\"] = g[target].shift(1).rolling(w, min_periods=int(w*0.6)).mean()\n",
    "        df_all[f\"roll_std_{w}\"]  = g[target].shift(1).rolling(w, min_periods=int(w*0.6)).std()\n",
    "    return df_all\n",
    "\n",
    "# ---------- 로드 ----------\n",
    "if not os.path.exists(META_PATH): raise FileNotFoundError(META_PATH)\n",
    "if not os.path.exists(MODEL_PATH): raise FileNotFoundError(MODEL_PATH)\n",
    "if not os.path.exists(CSV_TO_EVAL): raise FileNotFoundError(CSV_TO_EVAL)\n",
    "\n",
    "with open(META_PATH, \"r\", encoding=\"utf-8\") as f: \n",
    "    meta = json.load(f)\n",
    "TARGET_COL   = meta.get(\"target\", \"합산발전량(MWh)\")\n",
    "GROUP_KEYS   = meta.get(\"group_keys_used\", [])\n",
    "LAG_MAX      = int(meta.get(\"lags\", 168))\n",
    "ROLL_WINDOWS = list(meta.get(\"roll_windows\", [24,72,168]))\n",
    "TRAIN_PATH   = meta.get(\"train_path\")\n",
    "\n",
    "eval_df = read_csv_flex(CSV_TO_EVAL)\n",
    "dt_col = detect_datetime(eval_df)\n",
    "if dt_col is None: \n",
    "    raise ValueError(\"datetime 컬럼을 찾을 수 없습니다.\")\n",
    "eval_df = add_time_features(eval_df, dt_col)\n",
    "if GROUP_FILTER:\n",
    "    for k, v in GROUP_FILTER.items():\n",
    "        if k in eval_df.columns: \n",
    "            eval_df = eval_df[eval_df[k] == v]\n",
    "\n",
    "# 학습 마지막 168시간을 이어 붙여 동일 피처 재현\n",
    "if TRAIN_PATH and os.path.exists(TRAIN_PATH):\n",
    "    tr = read_csv_flex(TRAIN_PATH)\n",
    "    dt_tr = detect_datetime(tr)\n",
    "    if dt_tr is None: \n",
    "        raise ValueError(\"train_data에서 datetime 생성 실패\")\n",
    "    tr = add_time_features(tr, dt_tr)\n",
    "    keys = ensure_keys(eval_df, GROUP_KEYS)\n",
    "    if keys:\n",
    "        tail = tr.sort_values(keys + [dt_tr]).groupby(keys, as_index=False).tail(LAG_MAX)\n",
    "    else:\n",
    "        tail = tr.sort_values(dt_tr).tail(LAG_MAX)\n",
    "    common = list(set(tail.columns) & set(eval_df.columns))\n",
    "    base = pd.concat([tail[common], eval_df[common]], axis=0, ignore_index=True)\n",
    "else:\n",
    "    base = eval_df.copy()\n",
    "\n",
    "base = add_lags_and_rolls(base, \"__dt__\", GROUP_KEYS, TARGET_COL, LAG_MAX, ROLL_WINDOWS)\n",
    "eval_part = base.iloc[len(base) - len(eval_df):].copy()\n",
    "\n",
    "# ---------- 예측 ----------\n",
    "import joblib\n",
    "pipe = joblib.load(MODEL_PATH)\n",
    "X = eval_part.drop(columns=[c for c in [TARGET_COL, \"__dt__\"] if c in eval_part.columns], errors=\"ignore\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    y_pred = pipe.predict(X)\n",
    "\n",
    "pred = eval_part[[dt_col] + [k for k in ensure_keys(eval_part, GROUP_KEYS)]].copy()\n",
    "pred[\"y_pred\"] = y_pred\n",
    "if TARGET_COL in eval_part.columns:\n",
    "    pred[\"y_true\"] = eval_part[TARGET_COL].astype(float).values\n",
    "else:\n",
    "    raise ValueError(\"이상치 시각화는 실측 타깃 열이 필요합니다.\")\n",
    "\n",
    "# 여러 그룹이면 시간별 합계로 통합\n",
    "agg = pred.groupby(dt_col, as_index=False).sum(numeric_only=True).sort_values(dt_col)\n",
    "agg[\"resid\"] = agg[\"y_true\"] - agg[\"y_pred\"]\n",
    "\n",
    "# 점수: anom_score가 있으면 사용, 없으면 |잔차|\n",
    "anom_col = \"anom_score\"\n",
    "if anom_col in agg.columns and agg[anom_col].notna().any():\n",
    "    score = agg[anom_col].values\n",
    "else:\n",
    "    score = np.abs(agg[\"resid\"].values)\n",
    "\n",
    "thr = np.quantile(score[~np.isnan(score)], 1.0 - TOP_P)\n",
    "agg[\"is_outlier\"] = score >= thr\n",
    "\n",
    "# ---------- 색상 고정 ----------\n",
    "COLOR_TRUE = \"#1f77b4\"   # 파랑\n",
    "COLOR_PRED = \"#ff7f0e\"   # 주황\n",
    "COLOR_OUTL = \"#d62728\"   # 빨강\n",
    "\n",
    "# ---------- (A) 전체 플롯 ----------\n",
    "plt.figure(figsize=(18,5))\n",
    "plt.plot(agg[dt_col], agg[\"y_true\"], label=\"True\", linewidth=1.6, color=COLOR_TRUE)\n",
    "plt.plot(agg[dt_col], agg[\"y_pred\"], label=\"Pred\", linewidth=1.2, color=COLOR_PRED, alpha=0.85)\n",
    "out = agg[agg[\"is_outlier\"]]\n",
    "plt.scatter(out[dt_col], out[\"y_true\"], s=14, label=f\"Top {int(TOP_P*100)}% Outliers\",\n",
    "            zorder=5, color=COLOR_OUTL, edgecolors=\"none\")\n",
    "plt.title(f\"전체 결과 (Top {int(TOP_P*100)}% 이상치 강조)\")\n",
    "plt.xlabel(\"Time\"); plt.ylabel(\"Value\"); plt.legend()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ---------- (B) 주간 확대 플롯 ----------\n",
    "if WEEK_START is None:\n",
    "    start = agg[dt_col].min().normalize()\n",
    "else:\n",
    "    start = pd.to_datetime(WEEK_START).normalize()\n",
    "end = start + pd.Timedelta(days=7)\n",
    "\n",
    "week = agg[(agg[dt_col] >= start) & (agg[dt_col] < end)]\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.plot(week[dt_col], week[\"y_true\"], label=\"True\", linewidth=1.6, color=COLOR_TRUE)\n",
    "plt.plot(week[dt_col], week[\"y_pred\"], label=\"Pred\", linewidth=1.2, color=COLOR_PRED, alpha=0.9)\n",
    "wk_out = week[week[\"is_outlier\"]]\n",
    "plt.scatter(wk_out[dt_col], wk_out[\"y_true\"], s=18, label=\"Outlier\", zorder=6,\n",
    "            color=COLOR_OUTL, edgecolors=\"none\")\n",
    "plt.title(f\"주간 확대 (Week 1): {start.date()} ~ {end.date()}\")\n",
    "plt.xlabel(\"Time\"); plt.ylabel(\"Value\"); plt.legend()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f4b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Jupyter)",
   "language": "python",
   "name": "py311_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
