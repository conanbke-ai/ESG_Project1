{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113961b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TARGET scale] train: {'min': 0.0, 'q50': 0.0, 'max': 11752.5408}\n",
      "[TARGET scale] test : {'min': 0.0, 'q50': 0.0, 'max': 1355.379}\n",
      "ðŸ”§ ìž…ë ¥/íƒ€ê¹ƒ ë‹¨ìœ„ ë³´ì • ë°°ìˆ˜: rawâ‰ˆ255.73, chosen=100\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# XGBoost ì£¼ê°„í•™ìŠµ ë‹¤ìŒë‚  ì˜ˆì¸¡ (100 ì—í¬í¬, ì¡°ìš©í•œ ì¶œë ¥)\n",
    "# - train_data2.csv, test_data2.csv ì‚¬ìš©\n",
    "# - íƒ€ê¹ƒ: \"í•©ì‚°ë°œì „ëŸ‰(MWh)\" (íšŒê·€)\n",
    "# - ì¶œë ¥: ì˜ˆì¸¡ CSV, ëª¨ë¸ íŒŒì´í”„ë¼ì¸(pkl), ë©”íƒ€ JSON\n",
    "# - í¬í•¨: ìŠ¤ì¼€ì¼ ì ê²€, í…ŒìŠ¤íŠ¸ í”¼ì²˜/íƒ€ê¹ƒ ë‹¨ìœ„ ë³´ì •(ê°•ì œ), ì •ë ¬ ì¼ì¹˜ í‰ê°€\n",
    "# ==========================================\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# ---------------------------\n",
    "# 0. ì‚¬ìš©ìž ì„¤ì •\n",
    "# ---------------------------\n",
    "TRAIN_PATH = r\"C:\\ESG_Project1\\file\\merge_data\\train_data.csv\"\n",
    "TEST_PATH  = r\"C:\\ESG_Project1\\file\\merge_data\\test_data.csv\"\n",
    "TARGET_COL = \"í•©ì‚°ë°œì „ëŸ‰(MWh)\"\n",
    "GROUP_KEYS = [\"ë°œì „êµ¬ë¶„\", \"ì§€ì—­\", \"ì§€ì ë²ˆí˜¸\"]\n",
    "\n",
    "OUT_PREFIX = \"xgb_week2day\"\n",
    "RANDOM_STATE = 42\n",
    "EPOCHS = 100\n",
    "VAL_DAYS = 7\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "LAG_MAX = 168\n",
    "ROLL_WINDOWS = [24, 72, 168]\n",
    "\n",
    "# ---------------------------\n",
    "# 1. ìœ í‹¸\n",
    "# ---------------------------\n",
    "def read_csv_flex(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=\"utf-8\", low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"cp949\", low_memory=False)\n",
    "\n",
    "def detect_datetime(df: pd.DataFrame):\n",
    "    for c in [\"ì¼ì‹œ\", \"datetime\", \"timestamp\", \"DATE_TIME\", \"date_time\"]:\n",
    "        if c in df.columns:\n",
    "            dt = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "            if dt.notna().any():\n",
    "                df[\"__dt__\"] = dt\n",
    "                return \"__dt__\"\n",
    "    date_cands = [\"ì¼ìž\", \"date\"]\n",
    "    hour_cands = [\"ì‹œê°„\", \"hour\", \"HOUR\"]\n",
    "    date_col = next((c for c in date_cands if c in df.columns), None)\n",
    "    hour_col = next((c for c in hour_cands if c in df.columns), None)\n",
    "    if date_col and hour_col:\n",
    "        h = df[hour_col].astype(str).str.extract(r\"(\\d{1,2})\")[0].astype(float).clip(0, 23)\n",
    "        df[\"__dt__\"] = pd.to_datetime(df[date_col], errors=\"coerce\") + pd.to_timedelta(h, unit=\"h\")\n",
    "        return \"__dt__\"\n",
    "    return None\n",
    "\n",
    "def add_time_features(df: pd.DataFrame, dt_col: str):\n",
    "    df[\"hour\"] = df[dt_col].dt.hour\n",
    "    df[\"dayofweek\"] = df[dt_col].dt.dayofweek\n",
    "    df[\"month\"] = df[dt_col].dt.month\n",
    "    df[\"is_weekend\"] = (df[\"dayofweek\"] >= 5).astype(int)\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24.0)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24.0)\n",
    "    df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12.0)\n",
    "    df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12.0)\n",
    "    return df\n",
    "\n",
    "def ensure_keys(df, keys):\n",
    "    return [k for k in (keys or []) if k in df.columns]\n",
    "\n",
    "def add_lags_and_rolls(df_all: pd.DataFrame, dt_col: str, keys, target: str):\n",
    "    keys = ensure_keys(df_all, keys)\n",
    "    df_all = df_all.sort_values(keys + [dt_col] if keys else [dt_col]).copy()\n",
    "\n",
    "    # ëž˜ê·¸\n",
    "    lag_blocks = []\n",
    "    for lag in range(1, LAG_MAX + 1):\n",
    "        series = (df_all.groupby(keys)[target].shift(lag) if keys\n",
    "                  else df_all[target].shift(lag))\n",
    "        lag_blocks.append(series.rename(f\"lag_{lag}\"))\n",
    "    lag_df = pd.concat(lag_blocks, axis=1)\n",
    "\n",
    "    # ë¡¤ë§ (ì •ë³´ëˆ„ìˆ˜ ë°©ì§€: shift(1) í›„ rolling)\n",
    "    roll_mean_blocks, roll_std_blocks = [], []\n",
    "    for w in ROLL_WINDOWS:\n",
    "        if keys:\n",
    "            g = df_all.groupby(keys)[target].shift(1)\n",
    "            rm = g.rolling(w, min_periods=int(w*0.6)).mean()\n",
    "            rs = g.rolling(w, min_periods=int(w*0.6)).std()\n",
    "        else:\n",
    "            rm = df_all[target].shift(1).rolling(w, min_periods=int(w*0.6)).mean()\n",
    "            rs = df_all[target].shift(1).rolling(w, min_periods=int(w*0.6)).std()\n",
    "        roll_mean_blocks.append(rm.rename(f\"roll_mean_{w}\"))\n",
    "        roll_std_blocks.append(rs.rename(f\"roll_std_{w}\"))\n",
    "    roll_df = pd.concat(roll_mean_blocks + roll_std_blocks, axis=1)\n",
    "\n",
    "    df_all = pd.concat([df_all, lag_df, roll_df], axis=1)\n",
    "    return df_all\n",
    "\n",
    "def choose_tree_method():\n",
    "    try:\n",
    "        XGBRegressor(n_estimators=1, tree_method=\"gpu_hist\", predictor=\"gpu_predictor\").fit(\n",
    "            np.array([[0, 0]]), np.array([0.0])\n",
    "        )\n",
    "        return \"gpu_hist\", \"gpu_predictor\"\n",
    "    except Exception:\n",
    "        return \"hist\", \"auto\"\n",
    "\n",
    "def _q(s):\n",
    "    s = np.asarray(s, dtype=float)\n",
    "    s = s[np.isfinite(s)]\n",
    "    if s.size == 0:\n",
    "        return {\"min\": None, \"q50\": None, \"max\": None}\n",
    "    return dict(min=float(np.nanmin(s)), q50=float(np.nanmedian(s)), max=float(np.nanmax(s)))\n",
    "\n",
    "def _p(a, q):\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    a = a[np.isfinite(a)]\n",
    "    return np.percentile(a, q) if a.size else np.nan\n",
    "\n",
    "# ---------------------------\n",
    "# 2. ë°ì´í„° ë¡œë“œ\n",
    "# ---------------------------\n",
    "train = read_csv_flex(TRAIN_PATH)\n",
    "test  = read_csv_flex(TEST_PATH)\n",
    "\n",
    "dt_col_train = detect_datetime(train)\n",
    "dt_col_test  = detect_datetime(test)\n",
    "if dt_col_train is None or dt_col_test is None:\n",
    "    raise ValueError(\"datetime ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì¼ì‹œ ë˜ëŠ” ì¼ìž+ì‹œê°„ í•„ìš”)\")\n",
    "\n",
    "train = add_time_features(train, dt_col_train)\n",
    "test  = add_time_features(test, dt_col_test)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. ížˆìŠ¤í† ë¦¬ ê²°í•© í›„ ëž˜ê·¸/ë¡¤ë§ ìƒì„±\n",
    "# ---------------------------\n",
    "all_df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "all_df = add_lags_and_rolls(all_df, \"__dt__\", GROUP_KEYS, TARGET_COL)\n",
    "\n",
    "# ë¶„ë¦¬\n",
    "n_train = len(train)\n",
    "train_df = all_df.iloc[:n_train].copy()\n",
    "test_df  = all_df.iloc[n_train:].copy()\n",
    "\n",
    "# ---------------------------\n",
    "# 4. í…ŒìŠ¤íŠ¸ í”¼ì²˜/íƒ€ê¹ƒ ë‹¨ìœ„ ë³´ì • (ê°•ì œ ë²„ì „)\n",
    "#    í›ˆë ¨ íƒ€ê¹ƒ ë¶„í¬ vs í…ŒìŠ¤íŠ¸ lag_1/íƒ€ê¹ƒ ë¶„í¬ì˜ p95, max ë¹„ìœ¨ë¡œ ë°°ìˆ˜ ì‚°ì •\n",
    "# ---------------------------\n",
    "lag_cols  = [f\"lag_{i}\" for i in range(1, LAG_MAX + 1)]\n",
    "roll_cols = [c for c in test_df.columns if c.startswith(\"roll_mean_\") or c.startswith(\"roll_std_\")]\n",
    "cands = np.array([1, 10, 24, 100, 1000, 10000])  # í”í•œ ë‹¨ìœ„ ë°°ìˆ˜ í›„ë³´\n",
    "\n",
    "tr_p95 = _p(train_df[TARGET_COL].values, 95)\n",
    "tr_max = _p(train_df[TARGET_COL].values, 100)\n",
    "te_p95_lag1 = _p(test_df.get(\"lag_1\", pd.Series(np.nan, index=test_df.index)).values, 95)\n",
    "te_max_lag1 = _p(test_df.get(\"lag_1\", pd.Series(np.nan, index=test_df.index)).values, 100)\n",
    "\n",
    "ratios = []\n",
    "if np.isfinite(tr_p95) and np.isfinite(te_p95_lag1) and te_p95_lag1 > 0:\n",
    "    ratios.append(tr_p95 / te_p95_lag1)\n",
    "if np.isfinite(tr_max) and np.isfinite(te_max_lag1) and te_max_lag1 > 0:\n",
    "    ratios.append(tr_max / te_max_lag1)\n",
    "\n",
    "# lag_1ì´ ë¬´ì˜ë¯¸í•  ë•ŒëŠ” í…ŒìŠ¤íŠ¸ íƒ€ê¹ƒ ìžì²´ì™€ ë¹„êµ\n",
    "te_max_target = _p(test_df[TARGET_COL].values, 100)\n",
    "if (not ratios) and np.isfinite(tr_max) and np.isfinite(te_max_target) and te_max_target > 0:\n",
    "    ratios.append(tr_max / te_max_target)\n",
    "\n",
    "raw_factor = float(np.median(ratios)) if ratios else 1.0\n",
    "unit_factor = float(cands[np.argmin(np.abs(cands - raw_factor))])\n",
    "if not np.isfinite(unit_factor) or unit_factor <= 0:\n",
    "    unit_factor = 1.0\n",
    "\n",
    "print(\"[TARGET scale] train:\", _q(train_df[TARGET_COL].values))\n",
    "print(\"[TARGET scale] test :\", _q(test_df[TARGET_COL].values))\n",
    "print(f\"ðŸ”§ ìž…ë ¥/íƒ€ê¹ƒ ë‹¨ìœ„ ë³´ì • ë°°ìˆ˜: rawâ‰ˆ{raw_factor:.2f}, chosen={unit_factor:.0f}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ í”¼ì²˜ ë³´ì •: lag_* / roll_* Ã— unit_factor\n",
    "if unit_factor != 1.0:\n",
    "    scale_cols = [c for c in lag_cols + roll_cols if c in test_df.columns]\n",
    "    test_df.loc[:, scale_cols] = test_df.loc[:, scale_cols].astype(float) * unit_factor\n",
    "\n",
    "# ---------------------------\n",
    "# 5. í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬\n",
    "# ---------------------------\n",
    "valid_mask_train = train_df[lag_cols].notna().all(axis=1)\n",
    "X_train = train_df.loc[valid_mask_train].drop(columns=[TARGET_COL, \"__dt__\"], errors=\"ignore\")\n",
    "y_train = train_df.loc[valid_mask_train, TARGET_COL].astype(float)\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET_COL, \"__dt__\"], errors=\"ignore\")\n",
    "y_test = test_df[TARGET_COL] if TARGET_COL in test_df.columns else None\n",
    "\n",
    "cutoff = train_df[\"__dt__\"].max() - pd.Timedelta(days=VAL_DAYS)\n",
    "val_mask = train_df.loc[valid_mask_train, \"__dt__\"] >= cutoff\n",
    "X_tr, X_val = X_train.loc[~val_mask], X_train.loc[val_mask]\n",
    "y_tr, y_val = y_train.loc[~val_mask], y_train.loc[val_mask]\n",
    "\n",
    "# ---------------------------\n",
    "# 6. ì „ì²˜ë¦¬/ëª¨ë¸ êµ¬ì„± ë° í•™ìŠµ\n",
    "# ---------------------------\n",
    "tree_method, predictor = choose_tree_method()\n",
    "model = XGBRegressor(\n",
    "    n_estimators=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    max_depth=8,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    tree_method=tree_method,\n",
    "    predictor=predictor,\n",
    "    eval_metric=\"rmse\",\n",
    "    verbosity=0,\n",
    ")\n",
    "\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "num_tf = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "cat_tf = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)),\n",
    "])\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[(\"num\", num_tf, numeric_cols), (\"cat\", cat_tf, categorical_cols)],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3,\n",
    ")\n",
    "\n",
    "preprocess.fit(X_train)\n",
    "X_tr_prep  = preprocess.transform(X_tr)\n",
    "X_val_prep = preprocess.transform(X_val)\n",
    "\n",
    "print(f\"â–¶ ëª¨ë¸ í•™ìŠµ ì¤‘... (ë¼ìš´ë“œ={EPOCHS}, ê²€ì¦ êµ¬ê°„={VAL_DAYS}ì¼)\")\n",
    "model.fit(X_tr_prep, y_tr)\n",
    "\n",
    "val_pred = model.predict(X_val_prep)\n",
    "val_mae  = mean_absolute_error(y_val, val_pred)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "val_r2   = r2_score(y_val, val_pred)\n",
    "print(f\"[Validation] MAE={val_mae:.4f} | RMSE={val_rmse:.4f} | R2={val_r2:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. ì˜ˆì¸¡/ì €ìž¥\n",
    "# ---------------------------\n",
    "X_test_prep = preprocess.transform(X_test)\n",
    "test_pred = model.predict(X_test_prep)\n",
    "\n",
    "out_dir = os.path.dirname(TEST_PATH)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "pred_df = test.copy()\n",
    "pred_df[\"ì˜ˆì¸¡í•©ì‚°ë°œì „ëŸ‰(MWh)\"] = test_pred\n",
    "pred_path = os.path.join(out_dir, f\"{OUT_PREFIX}_predictions3.csv\")\n",
    "pred_df.to_csv(pred_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ìž¥: {pred_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 8. íŒŒì´í”„ë¼ì¸/ë©”íƒ€ ì €ìž¥ + í‰ê°€(ì •ë ¬ ì¼ì¹˜, ë‹¨ìœ„ ì¼ì¹˜)\n",
    "# ---------------------------\n",
    "pipe = Pipeline([(\"preprocess\", preprocess), (\"model\", model)])\n",
    "model_path = os.path.join(out_dir, f\"{OUT_PREFIX}_pipeline3.pkl\")\n",
    "joblib.dump(pipe, model_path)\n",
    "print(f\"âœ… ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì €ìž¥: {model_path}\")\n",
    "\n",
    "metrics = None\n",
    "if y_test is not None and y_test.notna().any():\n",
    "    eval_df = test_df[[\"__dt__\", *ensure_keys(test_df, GROUP_KEYS), TARGET_COL]].copy()\n",
    "    eval_df[\"y_true\"] = eval_df[TARGET_COL].astype(float) * unit_factor   # íƒ€ê¹ƒë„ ë³´ì •\n",
    "    eval_df[\"y_pred\"] = test_pred                                         # ì˜ˆì¸¡ì€ í”¼ì²˜ ë³´ì • ë°˜ì˜\n",
    "\n",
    "    valid_lags_mask_test = test_df[lag_cols].notna().all(axis=1)\n",
    "    mask_eval = np.isfinite(eval_df[\"y_true\"]) & valid_lags_mask_test\n",
    "    eval_df = eval_df[mask_eval].copy()\n",
    "\n",
    "    mae  = mean_absolute_error(eval_df[\"y_true\"], eval_df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(eval_df[\"y_true\"], eval_df[\"y_pred\"]))\n",
    "    r2   = r2_score(eval_df[\"y_true\"], eval_df[\"y_pred\"])\n",
    "    metrics = {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2), \"unit_factor\": float(unit_factor)}\n",
    "    print(f\"[Test] MAE={mae:.4f} | RMSE={rmse:.4f} | R2={r2:.4f}\")\n",
    "\n",
    "    dbg_path = os.path.join(out_dir, f\"{OUT_PREFIX}_debug_sample.csv\")\n",
    "    eval_df[[\"__dt__\", \"y_true\", \"y_pred\"]].head(10).to_csv(dbg_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"ðŸ§ª ë””ë²„ê·¸ ìƒ˜í”Œ ì €ìž¥: {dbg_path}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ í…ŒìŠ¤íŠ¸ íƒ€ê¹ƒì´ ì—†ì–´ í…ŒìŠ¤íŠ¸ ì§€í‘œë¥¼ ê³„ì‚°í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "meta = {\n",
    "    \"train_path\": TRAIN_PATH,\n",
    "    \"test_path\": TEST_PATH,\n",
    "    \"target\": TARGET_COL,\n",
    "    \"datetime_col\": \"__dt__\",\n",
    "    \"group_keys_used\": ensure_keys(train, GROUP_KEYS),\n",
    "    \"tree_method\": tree_method,\n",
    "    \"predictor\": predictor,\n",
    "    \"lags\": LAG_MAX,\n",
    "    \"roll_windows\": ROLL_WINDOWS,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"val_days\": VAL_DAYS,\n",
    "    \"numeric_cols\": numeric_cols,\n",
    "    \"categorical_cols\": categorical_cols,\n",
    "    \"metrics_val\": {\"MAE\": float(val_mae), \"RMSE\": float(val_rmse), \"R2\": float(val_r2)},\n",
    "    \"metrics_test\": metrics,\n",
    "}\n",
    "with open(os.path.join(out_dir, f\"{OUT_PREFIX}_meta3.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"â„¹ï¸ ë©”íƒ€ ì €ìž¥ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb71537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ì´ìƒì¹˜ íƒì§€ & ì‹œê°í™” (Top p%)\n",
    "# - unit_factor: train tailë§Œ ë³´ì •(/=)  â† â˜…ì¤‘ìš”\n",
    "# - y_true/y_predì—ëŠ” ì¶”ê°€ ê³±ì…ˆ ì—†ìŒ(ì¤‘ë³µ ë°©ì§€)\n",
    "# - 2024ë…„ ì´í›„ë§Œ ë¶„ì„/ì‹œê°í™”\n",
    "# ==========================================\n",
    "import os, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# ---------- ì„¤ì • ----------\n",
    "CSV_TO_EVAL = r\"C:\\ESG_Project1\\file\\merge_data\\test_data2.csv\"\n",
    "OUT_PREFIX  = \"xgb_week2day\"\n",
    "WORK_DIR    = os.path.dirname(CSV_TO_EVAL)\n",
    "MODEL_PATH  = os.path.join(WORK_DIR, f\"{OUT_PREFIX}_pipeline3.pkl\")\n",
    "META_PATH   = os.path.join(WORK_DIR, f\"{OUT_PREFIX}_meta3.json\")\n",
    "\n",
    "TOP_P = 0.01\n",
    "WEEK_START = \"2024-01-01\"\n",
    "\n",
    "def _set_korean_font():\n",
    "    try:\n",
    "        if os.name == \"nt\": matplotlib.rc(\"font\", family=\"Malgun Gothic\")\n",
    "        elif hasattr(os, \"uname\") and os.uname().sysname == \"Darwin\":\n",
    "            matplotlib.rc(\"font\", family=\"AppleGothic\")\n",
    "        matplotlib.rcParams[\"axes.unicode_minus\"] = False\n",
    "    except: pass\n",
    "_set_korean_font()\n",
    "\n",
    "def read_csv_flex(p):\n",
    "    try: return pd.read_csv(p, encoding=\"utf-8\", low_memory=False)\n",
    "    except UnicodeDecodeError: return pd.read_csv(p, encoding=\"cp949\", low_memory=False)\n",
    "\n",
    "def detect_datetime(df):\n",
    "    for c in [\"ì¼ì‹œ\",\"datetime\",\"timestamp\",\"DATE_TIME\",\"date_time\",\"__dt__\"]:\n",
    "        if c in df.columns:\n",
    "            dt = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "            if dt.notna().any():\n",
    "                df[\"__dt__\"] = dt\n",
    "                return \"__dt__\"\n",
    "    return None\n",
    "\n",
    "def add_time_features(df, dt_col):\n",
    "    df[\"hour\"] = df[dt_col].dt.hour\n",
    "    df[\"dayofweek\"] = df[dt_col].dt.dayofweek\n",
    "    df[\"month\"] = df[dt_col].dt.month\n",
    "    df[\"is_weekend\"] = (df[\"dayofweek\"] >= 5).astype(int)\n",
    "    df[\"hour_sin\"] = np.sin(2*np.pi*df[\"hour\"]/24)\n",
    "    df[\"hour_cos\"] = np.cos(2*np.pi*df[\"hour\"]/24)\n",
    "    df[\"month_sin\"] = np.sin(2*np.pi*df[\"month\"]/12)\n",
    "    df[\"month_cos\"] = np.cos(2*np.pi*df[\"month\"]/12)\n",
    "    return df\n",
    "\n",
    "def ensure_keys(df, keys): \n",
    "    return [k for k in (keys or []) if k in df.columns]\n",
    "\n",
    "def add_lags_and_rolls(df_all, dt_col, keys, target, lag_max, roll_windows):\n",
    "    keys = ensure_keys(df_all, keys)\n",
    "    df_all = df_all.sort_values(keys + [dt_col] if keys else [dt_col]).copy()\n",
    "    g = df_all.groupby(keys) if keys else df_all.assign(_=1).groupby(\"_\")\n",
    "    for lag in range(1, lag_max+1):\n",
    "        df_all[f\"lag_{lag}\"] = g[target].shift(lag)\n",
    "    for w in roll_windows:\n",
    "        df_all[f\"roll_mean_{w}\"] = g[target].shift(1).rolling(w, min_periods=int(w*0.6)).mean()\n",
    "        df_all[f\"roll_std_{w}\"]  = g[target].shift(1).rolling(w, min_periods=int(w*0.6)).std()\n",
    "    return df_all\n",
    "\n",
    "# ---------- ë©”íƒ€ ë¡œë“œ ----------\n",
    "with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "TARGET_COL   = meta.get(\"target\", \"í•©ì‚°ë°œì „ëŸ‰(MWh)\")\n",
    "GROUP_KEYS   = meta.get(\"group_keys_used\", [])\n",
    "LAG_MAX      = int(meta.get(\"lags\", 168))\n",
    "ROLL_WINDOWS = list(meta.get(\"roll_windows\", [24,72,168]))\n",
    "TRAIN_PATH   = meta.get(\"train_path\")\n",
    "# unit_factor ì—†ìœ¼ë©´ 1ë¡œ (ì¶”ì • í•„ìš”ì‹œ 10000 ë§ê³  1ë¡œ ë‘ëŠ”ê²Œ ì•ˆì „)\n",
    "UNIT_FACTOR  = float(meta.get(\"metrics_test\", {}).get(\"unit_factor\", 1.0) or 1.0)\n",
    "\n",
    "# ---------- í‰ê°€ ë°ì´í„° ----------\n",
    "eval_df = read_csv_flex(CSV_TO_EVAL)\n",
    "dt_col = detect_datetime(eval_df)\n",
    "if dt_col is None: raise ValueError(\"datetime ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "eval_df = add_time_features(eval_df, dt_col)\n",
    "\n",
    "# ---------- train tail ì´ì–´ë¶™ì´ê¸° (â˜…train tailë§Œ /= unit_factor) ----------\n",
    "if TRAIN_PATH and os.path.exists(TRAIN_PATH):\n",
    "    tr = read_csv_flex(TRAIN_PATH)\n",
    "    dt_tr = detect_datetime(tr)\n",
    "    if dt_tr is None: raise ValueError(\"train_dataì—ì„œ datetime ìƒì„± ì‹¤íŒ¨\")\n",
    "    tr = add_time_features(tr, dt_tr)\n",
    "    if TARGET_COL in tr.columns:\n",
    "        # í•™ìŠµ(maxâ‰ˆ55866), í…ŒìŠ¤íŠ¸(maxâ‰ˆ5.x)ì˜€ë‹¤ë©´ unit_factorâ‰ˆ10000\n",
    "        # â†’ test ìŠ¤ì¼€ì¼ì— ë§žì¶”ë ¤ë©´ train tailì„ ë‚˜ëˆ ì„œ ìž‘ê²Œ ë§Œë“¤ì–´ì•¼ í•¨.\n",
    "        tr[TARGET_COL] = tr[TARGET_COL].astype(float) / UNIT_FACTOR\n",
    "    keys = ensure_keys(eval_df, GROUP_KEYS)\n",
    "    tail = tr.sort_values(keys + [dt_tr]).groupby(keys, as_index=False).tail(LAG_MAX) if keys else tr.sort_values(dt_tr).tail(LAG_MAX)\n",
    "    common = list(set(tail.columns) & set(eval_df.columns))\n",
    "    base = pd.concat([tail[common], eval_df[common]], ignore_index=True)\n",
    "else:\n",
    "    base = eval_df.copy()\n",
    "\n",
    "# ë™ì¼ ë°©ì‹ìœ¼ë¡œ lag/roll ìƒì„±\n",
    "base = add_lags_and_rolls(base, \"__dt__\", GROUP_KEYS, TARGET_COL, LAG_MAX, ROLL_WINDOWS)\n",
    "eval_part = base.iloc[len(base) - len(eval_df):].copy()\n",
    "\n",
    "# ---------- ì˜ˆì¸¡ (y_true/y_predëŠ” ì¶”ê°€ ìŠ¤ì¼€ì¼ë§ ì—†ìŒ) ----------\n",
    "pipe = joblib.load(MODEL_PATH)\n",
    "X = eval_part.drop(columns=[c for c in [TARGET_COL, \"__dt__\"] if c in eval_part.columns], errors=\"ignore\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    y_pred = pipe.predict(X)\n",
    "\n",
    "pred = eval_part[[dt_col] + [k for k in ensure_keys(eval_part, GROUP_KEYS)]].copy()\n",
    "pred[\"y_pred\"] = y_pred\n",
    "pred[\"y_true\"] = eval_part[TARGET_COL].astype(float).values\n",
    "\n",
    "# ---------- 2024ë…„ ì´í›„ë§Œ ì§‘ê³„ ----------\n",
    "agg_full = pred.groupby(dt_col, as_index=False).sum(numeric_only=True).sort_values(dt_col)\n",
    "agg = agg_full[agg_full[dt_col] >= pd.Timestamp(\"2024-01-01\")].copy()\n",
    "\n",
    "# ---------- ì§€í‘œ/ì´ìƒì¹˜ ----------\n",
    "agg[\"resid\"] = agg[\"y_true\"] - agg[\"y_pred\"]\n",
    "mae  = mean_absolute_error(agg[\"y_true\"], agg[\"y_pred\"])\n",
    "rmse = np.sqrt(mean_squared_error(agg[\"y_true\"], agg[\"y_pred\"]))\n",
    "r2   = r2_score(agg[\"y_true\"], agg[\"y_pred\"])\n",
    "thr  = np.quantile(np.abs(agg[\"resid\"].values), 1 - TOP_P)\n",
    "agg[\"is_outlier\"] = np.abs(agg[\"resid\"]) >= thr\n",
    "\n",
    "print(f\"ì „ì²´ MAE {mae:.3f}  RMSE {rmse:.3f}  RÂ² {r2:.3f}\")\n",
    "print(f\"[INFO] ìƒìœ„ {int(TOP_P*100)}% ìž„ê³„ì¹˜: {thr:.3f}, ì´ìƒì¹˜ ìˆ˜: {int(agg['is_outlier'].sum())}\")\n",
    "\n",
    "# ---------- ì‹œê°í™” ----------\n",
    "COLOR_TRUE = \"#1f77b4\"; COLOR_PRED = \"#ff7f0e\"; COLOR_OUTL = \"#d62728\"\n",
    "\n",
    "plt.figure(figsize=(18,5))\n",
    "plt.plot(agg[dt_col], agg[\"y_true\"], label=\"True\", linewidth=1.5, color=COLOR_TRUE)\n",
    "plt.plot(agg[dt_col], agg[\"y_pred\"], label=\"Pred\", linewidth=1.3, color=COLOR_PRED, alpha=0.9)\n",
    "out = agg[agg[\"is_outlier\"]]\n",
    "plt.scatter(out[dt_col], out[\"y_true\"], s=15, color=COLOR_OUTL, label=f\"Top {int(TOP_P*100)}% Outliers\")\n",
    "plt.title(f\"ì „ì²´ ê²°ê³¼ (Top {int(TOP_P*100)}% ì´ìƒì¹˜ ê°•ì¡°, 2024~)\")\n",
    "plt.xlabel(\"Time\"); plt.ylabel(\"Value\"); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "start = pd.to_datetime(WEEK_START).normalize()\n",
    "end = start + pd.Timedelta(days=7)\n",
    "week = agg[(agg[dt_col] >= start) & (agg[dt_col] < end)]\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.plot(week[dt_col], week[\"y_true\"], label=\"True\", linewidth=1.5, color=COLOR_TRUE)\n",
    "plt.plot(week[dt_col], week[\"y_pred\"], label=\"Pred\", linewidth=1.3, color=COLOR_PRED, alpha=0.9)\n",
    "wk_out = week[week[\"is_outlier\"]]\n",
    "plt.scatter(wk_out[dt_col], wk_out[\"y_true\"], s=18, color=COLOR_OUTL, label=\"Outlier\")\n",
    "plt.title(f\"ì£¼ê°„ í™•ëŒ€: {start.date()} ~ {end.date()} (Top {int(TOP_P*100)}%)\")\n",
    "plt.xlabel(\"Time\"); plt.ylabel(\"Value\"); plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
