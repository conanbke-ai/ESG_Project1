{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113961b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TARGET scale] train: {'min': 0.0, 'q50': 0.0, 'max': 11752.5408}\n",
      "[TARGET scale] test : {'min': 0.0, 'q50': 0.0, 'max': 1355.379}\n",
      "🔧 입력/타깃 단위 보정 배수: raw≈255.73, chosen=100\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# XGBoost 주간학습 다음날 예측 (100 에포크, 조용한 출력)\n",
    "# - train_data2.csv, test_data2.csv 사용\n",
    "# - 타깃: \"합산발전량(MWh)\" (회귀)\n",
    "# - 출력: 예측 CSV, 모델 파이프라인(pkl), 메타 JSON\n",
    "# - 포함: 스케일 점검, 테스트 피처/타깃 단위 보정(강제), 정렬 일치 평가\n",
    "# ==========================================\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# ---------------------------\n",
    "# 0. 사용자 설정\n",
    "# ---------------------------\n",
    "TRAIN_PATH = r\"C:\\ESG_Project1\\file\\merge_data\\train_data.csv\"\n",
    "TEST_PATH  = r\"C:\\ESG_Project1\\file\\merge_data\\test_data.csv\"\n",
    "TARGET_COL = \"합산발전량(MWh)\"\n",
    "GROUP_KEYS = [\"발전구분\", \"지역\", \"지점번호\"]\n",
    "\n",
    "OUT_PREFIX = \"xgb_week2day\"\n",
    "RANDOM_STATE = 42\n",
    "EPOCHS = 100\n",
    "VAL_DAYS = 7\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "LAG_MAX = 168\n",
    "ROLL_WINDOWS = [24, 72, 168]\n",
    "\n",
    "# ---------------------------\n",
    "# 1. 유틸\n",
    "# ---------------------------\n",
    "def read_csv_flex(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=\"utf-8\", low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"cp949\", low_memory=False)\n",
    "\n",
    "def detect_datetime(df: pd.DataFrame):\n",
    "    for c in [\"일시\", \"datetime\", \"timestamp\", \"DATE_TIME\", \"date_time\"]:\n",
    "        if c in df.columns:\n",
    "            dt = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "            if dt.notna().any():\n",
    "                df[\"__dt__\"] = dt\n",
    "                return \"__dt__\"\n",
    "    date_cands = [\"일자\", \"date\"]\n",
    "    hour_cands = [\"시간\", \"hour\", \"HOUR\"]\n",
    "    date_col = next((c for c in date_cands if c in df.columns), None)\n",
    "    hour_col = next((c for c in hour_cands if c in df.columns), None)\n",
    "    if date_col and hour_col:\n",
    "        h = df[hour_col].astype(str).str.extract(r\"(\\d{1,2})\")[0].astype(float).clip(0, 23)\n",
    "        df[\"__dt__\"] = pd.to_datetime(df[date_col], errors=\"coerce\") + pd.to_timedelta(h, unit=\"h\")\n",
    "        return \"__dt__\"\n",
    "    return None\n",
    "\n",
    "def add_time_features(df: pd.DataFrame, dt_col: str):\n",
    "    df[\"hour\"] = df[dt_col].dt.hour\n",
    "    df[\"dayofweek\"] = df[dt_col].dt.dayofweek\n",
    "    df[\"month\"] = df[dt_col].dt.month\n",
    "    df[\"is_weekend\"] = (df[\"dayofweek\"] >= 5).astype(int)\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24.0)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24.0)\n",
    "    df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12.0)\n",
    "    df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12.0)\n",
    "    return df\n",
    "\n",
    "def ensure_keys(df, keys):\n",
    "    return [k for k in (keys or []) if k in df.columns]\n",
    "\n",
    "def add_lags_and_rolls(df_all: pd.DataFrame, dt_col: str, keys, target: str):\n",
    "    keys = ensure_keys(df_all, keys)\n",
    "    df_all = df_all.sort_values(keys + [dt_col] if keys else [dt_col]).copy()\n",
    "\n",
    "    # 래그\n",
    "    lag_blocks = []\n",
    "    for lag in range(1, LAG_MAX + 1):\n",
    "        series = (df_all.groupby(keys)[target].shift(lag) if keys\n",
    "                  else df_all[target].shift(lag))\n",
    "        lag_blocks.append(series.rename(f\"lag_{lag}\"))\n",
    "    lag_df = pd.concat(lag_blocks, axis=1)\n",
    "\n",
    "    # 롤링 (정보누수 방지: shift(1) 후 rolling)\n",
    "    roll_mean_blocks, roll_std_blocks = [], []\n",
    "    for w in ROLL_WINDOWS:\n",
    "        if keys:\n",
    "            g = df_all.groupby(keys)[target].shift(1)\n",
    "            rm = g.rolling(w, min_periods=int(w*0.6)).mean()\n",
    "            rs = g.rolling(w, min_periods=int(w*0.6)).std()\n",
    "        else:\n",
    "            rm = df_all[target].shift(1).rolling(w, min_periods=int(w*0.6)).mean()\n",
    "            rs = df_all[target].shift(1).rolling(w, min_periods=int(w*0.6)).std()\n",
    "        roll_mean_blocks.append(rm.rename(f\"roll_mean_{w}\"))\n",
    "        roll_std_blocks.append(rs.rename(f\"roll_std_{w}\"))\n",
    "    roll_df = pd.concat(roll_mean_blocks + roll_std_blocks, axis=1)\n",
    "\n",
    "    df_all = pd.concat([df_all, lag_df, roll_df], axis=1)\n",
    "    return df_all\n",
    "\n",
    "def choose_tree_method():\n",
    "    try:\n",
    "        XGBRegressor(n_estimators=1, tree_method=\"gpu_hist\", predictor=\"gpu_predictor\").fit(\n",
    "            np.array([[0, 0]]), np.array([0.0])\n",
    "        )\n",
    "        return \"gpu_hist\", \"gpu_predictor\"\n",
    "    except Exception:\n",
    "        return \"hist\", \"auto\"\n",
    "\n",
    "def _q(s):\n",
    "    s = np.asarray(s, dtype=float)\n",
    "    s = s[np.isfinite(s)]\n",
    "    if s.size == 0:\n",
    "        return {\"min\": None, \"q50\": None, \"max\": None}\n",
    "    return dict(min=float(np.nanmin(s)), q50=float(np.nanmedian(s)), max=float(np.nanmax(s)))\n",
    "\n",
    "def _p(a, q):\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    a = a[np.isfinite(a)]\n",
    "    return np.percentile(a, q) if a.size else np.nan\n",
    "\n",
    "# ---------------------------\n",
    "# 2. 데이터 로드\n",
    "# ---------------------------\n",
    "train = read_csv_flex(TRAIN_PATH)\n",
    "test  = read_csv_flex(TEST_PATH)\n",
    "\n",
    "dt_col_train = detect_datetime(train)\n",
    "dt_col_test  = detect_datetime(test)\n",
    "if dt_col_train is None or dt_col_test is None:\n",
    "    raise ValueError(\"datetime 컬럼을 찾을 수 없습니다. (일시 또는 일자+시간 필요)\")\n",
    "\n",
    "train = add_time_features(train, dt_col_train)\n",
    "test  = add_time_features(test, dt_col_test)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. 히스토리 결합 후 래그/롤링 생성\n",
    "# ---------------------------\n",
    "all_df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "all_df = add_lags_and_rolls(all_df, \"__dt__\", GROUP_KEYS, TARGET_COL)\n",
    "\n",
    "# 분리\n",
    "n_train = len(train)\n",
    "train_df = all_df.iloc[:n_train].copy()\n",
    "test_df  = all_df.iloc[n_train:].copy()\n",
    "\n",
    "# ---------------------------\n",
    "# 4. 테스트 피처/타깃 단위 보정 (강제 버전)\n",
    "#    훈련 타깃 분포 vs 테스트 lag_1/타깃 분포의 p95, max 비율로 배수 산정\n",
    "# ---------------------------\n",
    "lag_cols  = [f\"lag_{i}\" for i in range(1, LAG_MAX + 1)]\n",
    "roll_cols = [c for c in test_df.columns if c.startswith(\"roll_mean_\") or c.startswith(\"roll_std_\")]\n",
    "cands = np.array([1, 10, 24, 100, 1000, 10000])  # 흔한 단위 배수 후보\n",
    "\n",
    "tr_p95 = _p(train_df[TARGET_COL].values, 95)\n",
    "tr_max = _p(train_df[TARGET_COL].values, 100)\n",
    "te_p95_lag1 = _p(test_df.get(\"lag_1\", pd.Series(np.nan, index=test_df.index)).values, 95)\n",
    "te_max_lag1 = _p(test_df.get(\"lag_1\", pd.Series(np.nan, index=test_df.index)).values, 100)\n",
    "\n",
    "ratios = []\n",
    "if np.isfinite(tr_p95) and np.isfinite(te_p95_lag1) and te_p95_lag1 > 0:\n",
    "    ratios.append(tr_p95 / te_p95_lag1)\n",
    "if np.isfinite(tr_max) and np.isfinite(te_max_lag1) and te_max_lag1 > 0:\n",
    "    ratios.append(tr_max / te_max_lag1)\n",
    "\n",
    "# lag_1이 무의미할 때는 테스트 타깃 자체와 비교\n",
    "te_max_target = _p(test_df[TARGET_COL].values, 100)\n",
    "if (not ratios) and np.isfinite(tr_max) and np.isfinite(te_max_target) and te_max_target > 0:\n",
    "    ratios.append(tr_max / te_max_target)\n",
    "\n",
    "raw_factor = float(np.median(ratios)) if ratios else 1.0\n",
    "unit_factor = float(cands[np.argmin(np.abs(cands - raw_factor))])\n",
    "if not np.isfinite(unit_factor) or unit_factor <= 0:\n",
    "    unit_factor = 1.0\n",
    "\n",
    "print(\"[TARGET scale] train:\", _q(train_df[TARGET_COL].values))\n",
    "print(\"[TARGET scale] test :\", _q(test_df[TARGET_COL].values))\n",
    "print(f\"🔧 입력/타깃 단위 보정 배수: raw≈{raw_factor:.2f}, chosen={unit_factor:.0f}\")\n",
    "\n",
    "# 테스트 피처 보정: lag_* / roll_* × unit_factor\n",
    "if unit_factor != 1.0:\n",
    "    scale_cols = [c for c in lag_cols + roll_cols if c in test_df.columns]\n",
    "    test_df.loc[:, scale_cols] = test_df.loc[:, scale_cols].astype(float) * unit_factor\n",
    "\n",
    "# ---------------------------\n",
    "# 5. 학습/검증 분리\n",
    "# ---------------------------\n",
    "valid_mask_train = train_df[lag_cols].notna().all(axis=1)\n",
    "X_train = train_df.loc[valid_mask_train].drop(columns=[TARGET_COL, \"__dt__\"], errors=\"ignore\")\n",
    "y_train = train_df.loc[valid_mask_train, TARGET_COL].astype(float)\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET_COL, \"__dt__\"], errors=\"ignore\")\n",
    "y_test = test_df[TARGET_COL] if TARGET_COL in test_df.columns else None\n",
    "\n",
    "cutoff = train_df[\"__dt__\"].max() - pd.Timedelta(days=VAL_DAYS)\n",
    "val_mask = train_df.loc[valid_mask_train, \"__dt__\"] >= cutoff\n",
    "X_tr, X_val = X_train.loc[~val_mask], X_train.loc[val_mask]\n",
    "y_tr, y_val = y_train.loc[~val_mask], y_train.loc[val_mask]\n",
    "\n",
    "# ---------------------------\n",
    "# 6. 전처리/모델 구성 및 학습\n",
    "# ---------------------------\n",
    "tree_method, predictor = choose_tree_method()\n",
    "model = XGBRegressor(\n",
    "    n_estimators=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    max_depth=8,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    tree_method=tree_method,\n",
    "    predictor=predictor,\n",
    "    eval_metric=\"rmse\",\n",
    "    verbosity=0,\n",
    ")\n",
    "\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "num_tf = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "cat_tf = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)),\n",
    "])\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[(\"num\", num_tf, numeric_cols), (\"cat\", cat_tf, categorical_cols)],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3,\n",
    ")\n",
    "\n",
    "preprocess.fit(X_train)\n",
    "X_tr_prep  = preprocess.transform(X_tr)\n",
    "X_val_prep = preprocess.transform(X_val)\n",
    "\n",
    "print(f\"▶ 모델 학습 중... (라운드={EPOCHS}, 검증 구간={VAL_DAYS}일)\")\n",
    "model.fit(X_tr_prep, y_tr)\n",
    "\n",
    "val_pred = model.predict(X_val_prep)\n",
    "val_mae  = mean_absolute_error(y_val, val_pred)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "val_r2   = r2_score(y_val, val_pred)\n",
    "print(f\"[Validation] MAE={val_mae:.4f} | RMSE={val_rmse:.4f} | R2={val_r2:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. 예측/저장\n",
    "# ---------------------------\n",
    "X_test_prep = preprocess.transform(X_test)\n",
    "test_pred = model.predict(X_test_prep)\n",
    "\n",
    "out_dir = os.path.dirname(TEST_PATH)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "pred_df = test.copy()\n",
    "pred_df[\"예측합산발전량(MWh)\"] = test_pred\n",
    "pred_path = os.path.join(out_dir, f\"{OUT_PREFIX}_predictions3.csv\")\n",
    "pred_df.to_csv(pred_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ 예측 결과 저장: {pred_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 8. 파이프라인/메타 저장 + 평가(정렬 일치, 단위 일치)\n",
    "# ---------------------------\n",
    "pipe = Pipeline([(\"preprocess\", preprocess), (\"model\", model)])\n",
    "model_path = os.path.join(out_dir, f\"{OUT_PREFIX}_pipeline3.pkl\")\n",
    "joblib.dump(pipe, model_path)\n",
    "print(f\"✅ 모델 파이프라인 저장: {model_path}\")\n",
    "\n",
    "metrics = None\n",
    "if y_test is not None and y_test.notna().any():\n",
    "    eval_df = test_df[[\"__dt__\", *ensure_keys(test_df, GROUP_KEYS), TARGET_COL]].copy()\n",
    "    eval_df[\"y_true\"] = eval_df[TARGET_COL].astype(float) * unit_factor   # 타깃도 보정\n",
    "    eval_df[\"y_pred\"] = test_pred                                         # 예측은 피처 보정 반영\n",
    "\n",
    "    valid_lags_mask_test = test_df[lag_cols].notna().all(axis=1)\n",
    "    mask_eval = np.isfinite(eval_df[\"y_true\"]) & valid_lags_mask_test\n",
    "    eval_df = eval_df[mask_eval].copy()\n",
    "\n",
    "    mae  = mean_absolute_error(eval_df[\"y_true\"], eval_df[\"y_pred\"])\n",
    "    rmse = np.sqrt(mean_squared_error(eval_df[\"y_true\"], eval_df[\"y_pred\"]))\n",
    "    r2   = r2_score(eval_df[\"y_true\"], eval_df[\"y_pred\"])\n",
    "    metrics = {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2), \"unit_factor\": float(unit_factor)}\n",
    "    print(f\"[Test] MAE={mae:.4f} | RMSE={rmse:.4f} | R2={r2:.4f}\")\n",
    "\n",
    "    dbg_path = os.path.join(out_dir, f\"{OUT_PREFIX}_debug_sample.csv\")\n",
    "    eval_df[[\"__dt__\", \"y_true\", \"y_pred\"]].head(10).to_csv(dbg_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"🧪 디버그 샘플 저장: {dbg_path}\")\n",
    "else:\n",
    "    print(\"ℹ️ 테스트 타깃이 없어 테스트 지표를 계산하지 않았습니다.\")\n",
    "\n",
    "meta = {\n",
    "    \"train_path\": TRAIN_PATH,\n",
    "    \"test_path\": TEST_PATH,\n",
    "    \"target\": TARGET_COL,\n",
    "    \"datetime_col\": \"__dt__\",\n",
    "    \"group_keys_used\": ensure_keys(train, GROUP_KEYS),\n",
    "    \"tree_method\": tree_method,\n",
    "    \"predictor\": predictor,\n",
    "    \"lags\": LAG_MAX,\n",
    "    \"roll_windows\": ROLL_WINDOWS,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"val_days\": VAL_DAYS,\n",
    "    \"numeric_cols\": numeric_cols,\n",
    "    \"categorical_cols\": categorical_cols,\n",
    "    \"metrics_val\": {\"MAE\": float(val_mae), \"RMSE\": float(val_rmse), \"R2\": float(val_r2)},\n",
    "    \"metrics_test\": metrics,\n",
    "}\n",
    "with open(os.path.join(out_dir, f\"{OUT_PREFIX}_meta3.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"ℹ️ 메타 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb71537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 이상치 탐지 & 시각화 (Top p%)\n",
    "# - unit_factor: train tail만 보정(/=)  ← ★중요\n",
    "# - y_true/y_pred에는 추가 곱셈 없음(중복 방지)\n",
    "# - 2024년 이후만 분석/시각화\n",
    "# ==========================================\n",
    "import os, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# ---------- 설정 ----------\n",
    "CSV_TO_EVAL = r\"C:\\ESG_Project1\\file\\merge_data\\test_data2.csv\"\n",
    "OUT_PREFIX  = \"xgb_week2day\"\n",
    "WORK_DIR    = os.path.dirname(CSV_TO_EVAL)\n",
    "MODEL_PATH  = os.path.join(WORK_DIR, f\"{OUT_PREFIX}_pipeline3.pkl\")\n",
    "META_PATH   = os.path.join(WORK_DIR, f\"{OUT_PREFIX}_meta3.json\")\n",
    "\n",
    "TOP_P = 0.01\n",
    "WEEK_START = \"2024-01-01\"\n",
    "\n",
    "def _set_korean_font():\n",
    "    try:\n",
    "        if os.name == \"nt\": matplotlib.rc(\"font\", family=\"Malgun Gothic\")\n",
    "        elif hasattr(os, \"uname\") and os.uname().sysname == \"Darwin\":\n",
    "            matplotlib.rc(\"font\", family=\"AppleGothic\")\n",
    "        matplotlib.rcParams[\"axes.unicode_minus\"] = False\n",
    "    except: pass\n",
    "_set_korean_font()\n",
    "\n",
    "def read_csv_flex(p):\n",
    "    try: return pd.read_csv(p, encoding=\"utf-8\", low_memory=False)\n",
    "    except UnicodeDecodeError: return pd.read_csv(p, encoding=\"cp949\", low_memory=False)\n",
    "\n",
    "def detect_datetime(df):\n",
    "    for c in [\"일시\",\"datetime\",\"timestamp\",\"DATE_TIME\",\"date_time\",\"__dt__\"]:\n",
    "        if c in df.columns:\n",
    "            dt = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "            if dt.notna().any():\n",
    "                df[\"__dt__\"] = dt\n",
    "                return \"__dt__\"\n",
    "    return None\n",
    "\n",
    "def add_time_features(df, dt_col):\n",
    "    df[\"hour\"] = df[dt_col].dt.hour\n",
    "    df[\"dayofweek\"] = df[dt_col].dt.dayofweek\n",
    "    df[\"month\"] = df[dt_col].dt.month\n",
    "    df[\"is_weekend\"] = (df[\"dayofweek\"] >= 5).astype(int)\n",
    "    df[\"hour_sin\"] = np.sin(2*np.pi*df[\"hour\"]/24)\n",
    "    df[\"hour_cos\"] = np.cos(2*np.pi*df[\"hour\"]/24)\n",
    "    df[\"month_sin\"] = np.sin(2*np.pi*df[\"month\"]/12)\n",
    "    df[\"month_cos\"] = np.cos(2*np.pi*df[\"month\"]/12)\n",
    "    return df\n",
    "\n",
    "def ensure_keys(df, keys): \n",
    "    return [k for k in (keys or []) if k in df.columns]\n",
    "\n",
    "def add_lags_and_rolls(df_all, dt_col, keys, target, lag_max, roll_windows):\n",
    "    keys = ensure_keys(df_all, keys)\n",
    "    df_all = df_all.sort_values(keys + [dt_col] if keys else [dt_col]).copy()\n",
    "    g = df_all.groupby(keys) if keys else df_all.assign(_=1).groupby(\"_\")\n",
    "    for lag in range(1, lag_max+1):\n",
    "        df_all[f\"lag_{lag}\"] = g[target].shift(lag)\n",
    "    for w in roll_windows:\n",
    "        df_all[f\"roll_mean_{w}\"] = g[target].shift(1).rolling(w, min_periods=int(w*0.6)).mean()\n",
    "        df_all[f\"roll_std_{w}\"]  = g[target].shift(1).rolling(w, min_periods=int(w*0.6)).std()\n",
    "    return df_all\n",
    "\n",
    "# ---------- 메타 로드 ----------\n",
    "with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "TARGET_COL   = meta.get(\"target\", \"합산발전량(MWh)\")\n",
    "GROUP_KEYS   = meta.get(\"group_keys_used\", [])\n",
    "LAG_MAX      = int(meta.get(\"lags\", 168))\n",
    "ROLL_WINDOWS = list(meta.get(\"roll_windows\", [24,72,168]))\n",
    "TRAIN_PATH   = meta.get(\"train_path\")\n",
    "# unit_factor 없으면 1로 (추정 필요시 10000 말고 1로 두는게 안전)\n",
    "UNIT_FACTOR  = float(meta.get(\"metrics_test\", {}).get(\"unit_factor\", 1.0) or 1.0)\n",
    "\n",
    "# ---------- 평가 데이터 ----------\n",
    "eval_df = read_csv_flex(CSV_TO_EVAL)\n",
    "dt_col = detect_datetime(eval_df)\n",
    "if dt_col is None: raise ValueError(\"datetime 컬럼을 찾을 수 없습니다.\")\n",
    "eval_df = add_time_features(eval_df, dt_col)\n",
    "\n",
    "# ---------- train tail 이어붙이기 (★train tail만 /= unit_factor) ----------\n",
    "if TRAIN_PATH and os.path.exists(TRAIN_PATH):\n",
    "    tr = read_csv_flex(TRAIN_PATH)\n",
    "    dt_tr = detect_datetime(tr)\n",
    "    if dt_tr is None: raise ValueError(\"train_data에서 datetime 생성 실패\")\n",
    "    tr = add_time_features(tr, dt_tr)\n",
    "    if TARGET_COL in tr.columns:\n",
    "        # 학습(max≈55866), 테스트(max≈5.x)였다면 unit_factor≈10000\n",
    "        # → test 스케일에 맞추려면 train tail을 나눠서 작게 만들어야 함.\n",
    "        tr[TARGET_COL] = tr[TARGET_COL].astype(float) / UNIT_FACTOR\n",
    "    keys = ensure_keys(eval_df, GROUP_KEYS)\n",
    "    tail = tr.sort_values(keys + [dt_tr]).groupby(keys, as_index=False).tail(LAG_MAX) if keys else tr.sort_values(dt_tr).tail(LAG_MAX)\n",
    "    common = list(set(tail.columns) & set(eval_df.columns))\n",
    "    base = pd.concat([tail[common], eval_df[common]], ignore_index=True)\n",
    "else:\n",
    "    base = eval_df.copy()\n",
    "\n",
    "# 동일 방식으로 lag/roll 생성\n",
    "base = add_lags_and_rolls(base, \"__dt__\", GROUP_KEYS, TARGET_COL, LAG_MAX, ROLL_WINDOWS)\n",
    "eval_part = base.iloc[len(base) - len(eval_df):].copy()\n",
    "\n",
    "# ---------- 예측 (y_true/y_pred는 추가 스케일링 없음) ----------\n",
    "pipe = joblib.load(MODEL_PATH)\n",
    "X = eval_part.drop(columns=[c for c in [TARGET_COL, \"__dt__\"] if c in eval_part.columns], errors=\"ignore\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    y_pred = pipe.predict(X)\n",
    "\n",
    "pred = eval_part[[dt_col] + [k for k in ensure_keys(eval_part, GROUP_KEYS)]].copy()\n",
    "pred[\"y_pred\"] = y_pred\n",
    "pred[\"y_true\"] = eval_part[TARGET_COL].astype(float).values\n",
    "\n",
    "# ---------- 2024년 이후만 집계 ----------\n",
    "agg_full = pred.groupby(dt_col, as_index=False).sum(numeric_only=True).sort_values(dt_col)\n",
    "agg = agg_full[agg_full[dt_col] >= pd.Timestamp(\"2024-01-01\")].copy()\n",
    "\n",
    "# ---------- 지표/이상치 ----------\n",
    "agg[\"resid\"] = agg[\"y_true\"] - agg[\"y_pred\"]\n",
    "mae  = mean_absolute_error(agg[\"y_true\"], agg[\"y_pred\"])\n",
    "rmse = np.sqrt(mean_squared_error(agg[\"y_true\"], agg[\"y_pred\"]))\n",
    "r2   = r2_score(agg[\"y_true\"], agg[\"y_pred\"])\n",
    "thr  = np.quantile(np.abs(agg[\"resid\"].values), 1 - TOP_P)\n",
    "agg[\"is_outlier\"] = np.abs(agg[\"resid\"]) >= thr\n",
    "\n",
    "print(f\"전체 MAE {mae:.3f}  RMSE {rmse:.3f}  R² {r2:.3f}\")\n",
    "print(f\"[INFO] 상위 {int(TOP_P*100)}% 임계치: {thr:.3f}, 이상치 수: {int(agg['is_outlier'].sum())}\")\n",
    "\n",
    "# ---------- 시각화 ----------\n",
    "COLOR_TRUE = \"#1f77b4\"; COLOR_PRED = \"#ff7f0e\"; COLOR_OUTL = \"#d62728\"\n",
    "\n",
    "plt.figure(figsize=(18,5))\n",
    "plt.plot(agg[dt_col], agg[\"y_true\"], label=\"True\", linewidth=1.5, color=COLOR_TRUE)\n",
    "plt.plot(agg[dt_col], agg[\"y_pred\"], label=\"Pred\", linewidth=1.3, color=COLOR_PRED, alpha=0.9)\n",
    "out = agg[agg[\"is_outlier\"]]\n",
    "plt.scatter(out[dt_col], out[\"y_true\"], s=15, color=COLOR_OUTL, label=f\"Top {int(TOP_P*100)}% Outliers\")\n",
    "plt.title(f\"전체 결과 (Top {int(TOP_P*100)}% 이상치 강조, 2024~)\")\n",
    "plt.xlabel(\"Time\"); plt.ylabel(\"Value\"); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "start = pd.to_datetime(WEEK_START).normalize()\n",
    "end = start + pd.Timedelta(days=7)\n",
    "week = agg[(agg[dt_col] >= start) & (agg[dt_col] < end)]\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.plot(week[dt_col], week[\"y_true\"], label=\"True\", linewidth=1.5, color=COLOR_TRUE)\n",
    "plt.plot(week[dt_col], week[\"y_pred\"], label=\"Pred\", linewidth=1.3, color=COLOR_PRED, alpha=0.9)\n",
    "wk_out = week[week[\"is_outlier\"]]\n",
    "plt.scatter(wk_out[dt_col], wk_out[\"y_true\"], s=18, color=COLOR_OUTL, label=\"Outlier\")\n",
    "plt.title(f\"주간 확대: {start.date()} ~ {end.date()} (Top {int(TOP_P*100)}%)\")\n",
    "plt.xlabel(\"Time\"); plt.ylabel(\"Value\"); plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
