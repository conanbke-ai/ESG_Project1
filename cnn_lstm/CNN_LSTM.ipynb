{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db5f27a",
   "metadata": {},
   "source": [
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18f272bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.5.1\n",
      "CUDA 사용 가능: True\n",
      "GPU: NVIDIA GeForce RTX 4060\n",
      "tensor device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA 사용 가능:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# 간단 실행 테스트\n",
    "x = torch.randn(3, 3).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"tensor device:\", x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86a73458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "Train windows: 2553373 | Test windows: 687034\n",
      "Warm up done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c\\anaconda3\\envs\\python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001  Train Loss 1.071786\n",
      "Epoch 005  Train Loss 0.831281\n",
      "Epoch 010  Train Loss 0.649347\n",
      "Epoch 015  Train Loss 0.558094\n",
      "Epoch 020  Train Loss 0.513932\n",
      "Epoch 025  Train Loss 0.489516\n",
      "Epoch 030  Train Loss 0.419294\n",
      "Epoch 035  Train Loss 0.401921\n",
      "Epoch 040  Train Loss 0.386459\n",
      "Epoch 045  Train Loss 0.373088\n",
      "Epoch 050  Train Loss 0.364975\n",
      "Epoch 055  Train Loss 0.357474\n",
      "Epoch 060  Train Loss 0.349400\n",
      "Epoch 065  Train Loss 0.343872\n",
      "Epoch 070  Train Loss 0.339237\n",
      "Epoch 075  Train Loss 0.333561\n",
      "Epoch 080  Train Loss 0.331234\n",
      "Epoch 085  Train Loss 0.326511\n",
      "Epoch 090  Train Loss 0.301754\n",
      "Epoch 095  Train Loss 0.296242\n",
      "Epoch 100  Train Loss 0.293298\n",
      "saturation_rate (pred_log >= MAX_LOG_Y): 0.00243001074182646\n",
      "train_max: 6676.049867000241 test_max: 45003.39261600046\n",
      "\n",
      "전체 MAE 275.284  RMSE 2268.349  R² 0.294\n",
      "\n",
      "지평별 RMSE\n",
      "T+01h: 2205.597\n",
      "T+02h: 2187.195\n",
      "T+03h: 2254.214\n",
      "T+04h: 2198.926\n",
      "T+05h: 2261.098\n",
      "T+06h: 2287.509\n",
      "T+07h: 2247.171\n",
      "T+08h: 2323.362\n",
      "T+09h: 2292.444\n",
      "T+10h: 2276.227\n",
      "T+11h: 2291.895\n",
      "T+12h: 2315.571\n",
      "T+13h: 2296.808\n",
      "T+14h: 2288.797\n",
      "T+15h: 2293.469\n",
      "T+16h: 2241.334\n",
      "T+17h: 2283.318\n",
      "T+18h: 2324.109\n",
      "T+19h: 2325.887\n",
      "T+20h: 2277.476\n",
      "T+21h: 2249.401\n",
      "T+22h: 2259.444\n",
      "T+23h: 2209.930\n",
      "T+24h: 2241.105\n",
      "\n",
      "모델 가중치 저장 완료 → ./models\\cnn_lstm_best_20251028_182041.pt\n",
      "메타데이터 저장 완료 → ./models\\cnn_lstm_meta.json\n"
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# CNN-LSTM (R² 0.8 목표)\n",
    "# 메모리 효율형 윈도우 + RandomSampler + 체크 저장\n",
    "# (출력 상한 클리핑 제거 버전)\n",
    "# ===================================================\n",
    "\n",
    "import os, platform, json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- 설정 ----------\n",
    "TRAIN_CSV = r\"C:\\ESG_Project1\\file\\merge_data\\train_data.csv\"\n",
    "TEST_CSV  = r\"C:\\ESG_Project1\\file\\merge_data\\test_data.csv\"\n",
    "\n",
    "SEQ_LEN, HORIZON = 168, 24\n",
    "BATCH, EPOCHS, LR = 256, 100, 1e-3\n",
    "\n",
    "PEAK_RANGE  = (8, 18)\n",
    "PEAK_WEIGHT = 6.0\n",
    "\n",
    "WINDOW_STEP = 1\n",
    "LIMIT_PER_PLANT = None\n",
    "\n",
    "MAX_STEPS_PER_EPOCH = 2000\n",
    "EFFECTIVE_SAMPLES   = BATCH * MAX_STEPS_PER_EPOCH\n",
    "\n",
    "SAVE_DIR = \"./models\"; os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", DEVICE)\n",
    "\n",
    "# ---------- 폰트 ----------\n",
    "sysname = platform.system()\n",
    "if sysname == \"Windows\": plt.rcParams[\"font.family\"] = \"Malgun Gothic\"\n",
    "elif sysname == \"Darwin\": plt.rcParams[\"font.family\"] = \"AppleGothic\"\n",
    "else:\n",
    "    os.system(\"apt-get install -y fonts-nanum > /dev/null 2>&1\")\n",
    "    plt.rcParams[\"font.family\"] = \"NanumGothic\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# ---------- 유틸 ----------\n",
    "def read_csv_auto(path):\n",
    "    try: return pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "    except UnicodeDecodeError: return pd.read_csv(path, encoding=\"cp949\")\n",
    "\n",
    "def find_col(df, candidates):\n",
    "    cols = df.columns\n",
    "    for c in candidates:\n",
    "        if c in cols: return c\n",
    "    low = {c.lower(): c for c in cols}\n",
    "    for c in candidates:\n",
    "        if c.lower() in low: return low[c.lower()]\n",
    "    raise KeyError(f\"필수 컬럼 누락: {candidates} / 실제: {list(df.columns)}\")\n",
    "\n",
    "def to_numeric(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# ---------- 데이터 로드 전처리 ----------\n",
    "train_df, test_df = read_csv_auto(TRAIN_CSV), read_csv_auto(TEST_CSV)\n",
    "COL_TIME  = find_col(train_df, [\"일시\",\"datetime\",\"time\"])\n",
    "COL_PLANT = find_col(train_df, [\"발전구분\",\"발전기\",\"호기\",\"plant\"])\n",
    "COL_Y     = find_col(train_df, [\"합산발전량(MWh)\",\"합산발전량\",\"발전량\",\"target\"])\n",
    "CAND_NUM = [\"기온(℃)\",\"기온\",\"강수량(mm)\",\"강수량\",\"일조(hr)\",\"일조\",\n",
    "            \"일사(MJ/m2)\",\"일사\",\"지점번호\",\"지점\",\"station\",\"stn\"]\n",
    "CAT_COLS = [c for c in [\"지역\"] if c in train_df.columns]\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    df[COL_TIME] = pd.to_datetime(df[COL_TIME])\n",
    "    to_numeric(df, [COL_Y] + [c for c in CAND_NUM if c in df.columns])\n",
    "    df = df.dropna(subset=[COL_Y]).sort_values([COL_PLANT, COL_TIME])\n",
    "    for c in [COL_Y] + [c for c in CAND_NUM if c in df.columns]:\n",
    "        df[c] = np.nan_to_num(df[c], nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        df[c] = np.clip(df[c], 0, df[c].quantile(0.999))\n",
    "    return df\n",
    "\n",
    "train_df, test_df = preprocess(train_df), preprocess(test_df)\n",
    "NUM_FEATS = [c for c in CAND_NUM if c in train_df.columns]\n",
    "\n",
    "# ---------- 시간 특성 라그 ----------\n",
    "def add_time_features(df):\n",
    "    df = df.copy()\n",
    "    hour = df[COL_TIME].dt.hour.values\n",
    "    doy  = df[COL_TIME].dt.dayofyear.values\n",
    "    df[\"hour_sin\"] = np.sin(2*np.pi*hour/24)\n",
    "    df[\"hour_cos\"] = np.cos(2*np.pi*hour/24)\n",
    "    df[\"doy_sin\"]  = np.sin(2*np.pi*doy/365.25)\n",
    "    df[\"doy_cos\"]  = np.cos(2*np.pi*doy/365.25)\n",
    "    return df\n",
    "\n",
    "def add_lag(df):\n",
    "    df = df.copy().sort_values([COL_PLANT, COL_TIME])\n",
    "    g = df.groupby(COL_PLANT, group_keys=False)\n",
    "    df[\"lag_24\"]      = g[COL_Y].shift(24)\n",
    "    df[\"lag_168\"]     = g[COL_Y].shift(168)\n",
    "    df[\"lag_24_mean\"] = g[COL_Y].rolling(24).mean().reset_index(level=0, drop=True)\n",
    "    df[\"roll_mean_6\"]  = g[COL_Y].rolling(6).mean().reset_index(level=0, drop=True)\n",
    "    df[\"roll_mean_12\"] = g[COL_Y].rolling(12).mean().reset_index(level=0, drop=True)\n",
    "    return df.fillna(0)\n",
    "\n",
    "train_df, test_df = add_time_features(train_df), add_time_features(test_df)\n",
    "train_df, test_df = add_lag(train_df), add_lag(test_df)\n",
    "\n",
    "for c in [\"hour_sin\",\"hour_cos\",\"doy_sin\",\"doy_cos\",\"lag_24\",\"lag_168\",\n",
    "          \"lag_24_mean\",\"roll_mean_6\",\"roll_mean_12\"]:\n",
    "    if c not in NUM_FEATS:\n",
    "        NUM_FEATS.append(c)\n",
    "\n",
    "# ---------- 발전소별 정규화 ----------\n",
    "def normalize_per_plant(df, cols):\n",
    "    df = df.copy()\n",
    "    for p, sub in df.groupby(COL_PLANT):\n",
    "        mean, std = sub[cols].mean(), sub[cols].std()\n",
    "        idx = sub.index\n",
    "        df.loc[idx, cols] = (sub[cols] - mean) / (std + 1e-6)\n",
    "    return df\n",
    "\n",
    "train_df = normalize_per_plant(train_df, NUM_FEATS)\n",
    "test_df  = normalize_per_plant(test_df, NUM_FEATS)\n",
    "\n",
    "# ---------- 지역 원핫 ----------\n",
    "def one_hot_fit_transform(train_df, test_df, cat_cols):\n",
    "    if not cat_cols: return train_df, test_df, []\n",
    "    cats = {c: sorted(train_df[c].dropna().astype(str).unique()) for c in cat_cols}\n",
    "    def ohe(df):\n",
    "        out = df.copy()\n",
    "        for c in cat_cols:\n",
    "            for v in cats[c]:\n",
    "                out[f\"{c}={v}\"] = (out[c].astype(str) == v).astype(np.float32)\n",
    "        return out\n",
    "    return ohe(train_df), ohe(test_df), [f\"{c}={v}\" for c in cat_cols for v in cats[c]]\n",
    "\n",
    "train_df, test_df, OHE_FEATS = one_hot_fit_transform(train_df, test_df, CAT_COLS)\n",
    "\n",
    "# ---------- 스케일링 ----------\n",
    "x_scaler = MinMaxScaler()\n",
    "Xtr_num = x_scaler.fit_transform(train_df[NUM_FEATS].values.astype(np.float32))\n",
    "Xte_num = x_scaler.transform(test_df[NUM_FEATS].values.astype(np.float32))\n",
    "if OHE_FEATS:\n",
    "    Xtr = np.hstack([Xtr_num, train_df[OHE_FEATS].values.astype(np.float32)])\n",
    "    Xte = np.hstack([Xte_num,  test_df[OHE_FEATS].values.astype(np.float32)])\n",
    "else:\n",
    "    Xtr, Xte = Xtr_num, Xte_num\n",
    "\n",
    "ytr = np.log1p(np.maximum(train_df[[COL_Y]].values.astype(np.float32), 0)).ravel()\n",
    "yte = np.log1p(np.maximum(test_df [[COL_Y]].values.astype(np.float32), 0)).ravel()\n",
    "MAX_LOG_Y = float(np.log1p(train_df[COL_Y].quantile(0.999)))  # 진단용 유지\n",
    "\n",
    "plants_tr, plants_te = train_df[COL_PLANT].values, test_df[COL_PLANT].values\n",
    "\n",
    "# ===================================================\n",
    "# 메모리 효율형 윈도우 Dataset\n",
    "# ===================================================\n",
    "class WindowedDataset(Dataset):\n",
    "    def __init__(self, X_2d, y_1d, starts, seq_len=SEQ_LEN, horizon=HORIZON):\n",
    "        self.X, self.y = X_2d, y_1d\n",
    "        self.starts = starts\n",
    "        self.seq_len, self.horizon = seq_len, horizon\n",
    "    def __len__(self):\n",
    "        return len(self.starts)\n",
    "    def __getitem__(self, i):\n",
    "        s = int(self.starts[i])\n",
    "        x = self.X[s:s+self.seq_len, :]\n",
    "        y = self.y[s+self.seq_len:s+self.seq_len+self.horizon]\n",
    "        return torch.from_numpy(x).float(), torch.from_numpy(y).float()\n",
    "\n",
    "def make_start_positions(group, seq_len=SEQ_LEN, horizon=HORIZON, step=1, limit_per_group=None):\n",
    "    starts_all = []\n",
    "    for g in pd.unique(group):\n",
    "        idx = np.where(group == g)[0]\n",
    "        if len(idx) < seq_len + horizon:\n",
    "            continue\n",
    "        starts = idx[: len(idx) - (seq_len + horizon) + 1 : step]\n",
    "        if limit_per_group is not None and len(starts) > limit_per_group:\n",
    "            sel = np.linspace(0, len(starts)-1, num=limit_per_group, dtype=int)\n",
    "            starts = starts[sel]\n",
    "        starts_all.append(starts.astype(np.int32))\n",
    "    return np.concatenate(starts_all) if starts_all else np.empty((0,), dtype=np.int32)\n",
    "\n",
    "starts_tr = make_start_positions(plants_tr, step=WINDOW_STEP, limit_per_group=LIMIT_PER_PLANT)\n",
    "starts_te = make_start_positions(plants_te, step=1,            limit_per_group=None)\n",
    "\n",
    "train_ds = WindowedDataset(Xtr, ytr, starts_tr, SEQ_LEN, HORIZON)\n",
    "test_ds  = WindowedDataset(Xte, yte, starts_te, SEQ_LEN, HORIZON)\n",
    "\n",
    "print(\"Train windows:\", len(train_ds), \"| Test windows:\", len(test_ds))\n",
    "\n",
    "# ---------- DataLoader ----------\n",
    "sampler = RandomSampler(\n",
    "    train_ds,\n",
    "    replacement=True,\n",
    "    num_samples=int(EFFECTIVE_SAMPLES)\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH,\n",
    "    sampler=sampler,\n",
    "    drop_last=False,\n",
    "    pin_memory=(DEVICE.type == \"cuda\"),\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# ===================================================\n",
    "# 모델\n",
    "# ===================================================\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, n_features, horizon=24, dropout_p=0.25):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(n_features, 128, kernel_size=3, padding=1)\n",
    "        self.bn   = nn.BatchNorm1d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(128, 128, num_layers=2, batch_first=True, dropout=dropout_p)\n",
    "        self.fc1  = nn.Linear(128, 128)\n",
    "        self.drop = nn.Dropout(p=dropout_p)\n",
    "        self.fc2  = nn.Linear(128, horizon)\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.relu(self.bn(self.conv(x)))\n",
    "        x = x.transpose(1,2)\n",
    "        out,_ = self.lstm(x)\n",
    "        h = self.drop(self.relu(self.fc1(out[:,-1,:])))\n",
    "        y = torch.nn.functional.softplus(self.fc2(h))\n",
    "        return y  # ← 상한 클리핑 제거\n",
    "\n",
    "n_features = Xtr.shape[-1]\n",
    "model = CNNLSTM(n_features=n_features, horizon=HORIZON).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "huber = nn.SmoothL1Loss(beta=1.0)\n",
    "\n",
    "# ---------- 피크 가중 벡터 ----------\n",
    "hours = np.arange(HORIZON)\n",
    "peak_mask = (hours >= PEAK_RANGE[0]) & (hours < PEAK_RANGE[1])\n",
    "peak_w = torch.ones(HORIZON, device=DEVICE)\n",
    "peak_w[peak_mask] = PEAK_WEIGHT\n",
    "\n",
    "# ---------- 워밍업(선택) ----------\n",
    "with torch.no_grad():\n",
    "    warm_loader = DataLoader(train_ds, batch_size=min(32, BATCH), shuffle=False, num_workers=0)\n",
    "    try:\n",
    "        xb, yb = next(iter(warm_loader))\n",
    "        _ = model(xb.to(DEVICE))\n",
    "        print(\"Warm up done.\")\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "# ===================================================\n",
    "# 학습\n",
    "# ===================================================\n",
    "best = float('inf'); bad = 0; es_patience = 10\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train(); losses=[]\n",
    "    for step, (xb, yb) in enumerate(train_loader, start=1):\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        se = (pred - yb)**2\n",
    "        loss_rmse  = torch.sqrt(torch.clamp((se * peak_w.view(1,-1)).mean(), min=1e-12))\n",
    "        loss_huber = huber(pred, yb)\n",
    "        loss = 0.7*loss_rmse + 0.3*loss_huber\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        if step >= MAX_STEPS_PER_EPOCH:\n",
    "            break\n",
    "\n",
    "    epoch_loss = float(np.mean(losses)) if losses else np.nan\n",
    "    scheduler.step(epoch_loss if np.isfinite(epoch_loss) else best)\n",
    "\n",
    "    if ep % 5 == 0 or ep == 1:\n",
    "        print(f\"Epoch {ep:03d}  Train Loss {epoch_loss:.6f}\")\n",
    "\n",
    "    if np.isfinite(epoch_loss) and epoch_loss < best - 1e-4:\n",
    "        best = epoch_loss; bad = 0\n",
    "    else:\n",
    "        bad += 1\n",
    "    if bad >= es_patience:\n",
    "        print(f\"Early stop at epoch {ep} (best {best:.6f})\")\n",
    "        break\n",
    "\n",
    "# ===================================================\n",
    "# 검증\n",
    "# ===================================================\n",
    "model.eval()\n",
    "pred_log_list, y_log_list = [], []\n",
    "with torch.no_grad():\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH, shuffle=False,\n",
    "                             drop_last=False, pin_memory=(DEVICE.type == \"cuda\"), num_workers=0)\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        pred_log_list.append(model(xb).cpu().numpy())\n",
    "        y_log_list.append(yb.numpy())\n",
    "\n",
    "pred_log = np.concatenate(pred_log_list, axis=0).ravel()\n",
    "y_log   = np.concatenate(y_log_list, axis=0).ravel()\n",
    "\n",
    "# 진단용: 상한 포화율(이전 버전 영향 확인)\n",
    "print(\"saturation_rate (pred_log >= MAX_LOG_Y):\", float(np.mean(pred_log >= MAX_LOG_Y)))\n",
    "\n",
    "# 원단위 복원(클리핑 제거)\n",
    "y_true = np.expm1(y_log)\n",
    "y_pred = np.expm1(pred_log)\n",
    "\n",
    "# 분포 시프트 점검\n",
    "print(\"train_max:\", float(train_df[COL_Y].max()), \"test_max:\", float(test_df[COL_Y].max()))\n",
    "\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "r2   = r2_score(y_true, y_pred)\n",
    "print(f\"\\n전체 MAE {mae:.3f}  RMSE {rmse:.3f}  R² {r2:.3f}\")\n",
    "\n",
    "H = HORIZON\n",
    "y_true_2d = y_true.reshape(-1, H)\n",
    "y_pred_2d = y_pred.reshape(-1, H)\n",
    "print(\"\\n지평별 RMSE\")\n",
    "for h in range(H):\n",
    "    rmse_h = np.sqrt(mean_squared_error(y_true_2d[:, h], y_pred_2d[:, h]))\n",
    "    print(f\"T+{h+1:02d}h: {rmse_h:.3f}\")\n",
    "\n",
    "# ===================================================\n",
    "# 저장\n",
    "# ===================================================\n",
    "MODEL_PATH = os.path.join(SAVE_DIR, f\"cnn_lstm_best_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt\")\n",
    "META_PATH  = os.path.join(SAVE_DIR, \"cnn_lstm_meta.json\")\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"\\n모델 가중치 저장 완료 → {MODEL_PATH}\")\n",
    "\n",
    "meta = {\n",
    "    \"train_csv\": TRAIN_CSV,\n",
    "    \"test_csv\": TEST_CSV,\n",
    "    \"seq_len\": SEQ_LEN,\n",
    "    \"horizon\": HORIZON,\n",
    "    \"batch\": BATCH,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"lr\": LR,\n",
    "    \"device\": str(DEVICE),\n",
    "    \"features_num\": NUM_FEATS,\n",
    "    \"features_ohe\": OHE_FEATS,\n",
    "    \"scaler_min\": x_scaler.data_min_.tolist(),\n",
    "    \"scaler_max\": x_scaler.data_max_.tolist(),\n",
    "    \"performance\": {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)},\n",
    "    \"windows\": {\"train\": int(len(train_ds)), \"test\": int(len(test_ds))},\n",
    "    \"sampler\": {\"max_steps_per_epoch\": MAX_STEPS_PER_EPOCH, \"effective_samples\": int(EFFECTIVE_SAMPLES)},\n",
    "    \"window_step\": WINDOW_STEP,\n",
    "    \"limit_per_plant\": LIMIT_PER_PLANT,\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "with open(META_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"메타데이터 저장 완료 → {META_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "329269a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xte_seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m model.eval(); preds, trues = [], []\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mXte_seq\u001b[49m), BATCH):\n\u001b[32m     21\u001b[39m         xb = torch.from_numpy(Xte_seq[i:i+BATCH]).to(DEVICE)\n\u001b[32m     22\u001b[39m         p  = model(xb).cpu().numpy()\n",
      "\u001b[31mNameError\u001b[39m: name 'Xte_seq' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ✅ CNN-LSTM 예측 결과 기반 이상치 탐지 + 전체/주간 시각화 (통합)\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, platform\n",
    "\n",
    "# ---------- 설정 ----------\n",
    "OUTLIER_FRAC = 0.01     # 상위 1% 이상치\n",
    "WEEK_MODE    = \"calendar\"   # \"calendar\", \"rolling\", or \"index\"\n",
    "WEEK_KEY     = \"2024-12-29\" # 주간 확대 기준일\n",
    "WEEK_RULE    = \"W-MON\"      # 월~일 기준\n",
    "SAVE_DIR     = \"./models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- 예측/역변환 ----------\n",
    "model.eval(); preds, trues = [], []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(Xte_seq), BATCH):\n",
    "        xb = torch.from_numpy(Xte_seq[i:i+BATCH]).to(DEVICE)\n",
    "        p  = model(xb).cpu().numpy()\n",
    "        preds.append(p); trues.append(Yte_seq[i:i+BATCH])\n",
    "preds = np.concatenate(preds); trues = np.concatenate(trues)\n",
    "preds = np.clip(preds, 0.0, MAX_LOG_Y)\n",
    "preds_inv = np.expm1(preds); trues_inv = np.expm1(trues)\n",
    "preds_inv = np.maximum(preds_inv, 0.0)\n",
    "\n",
    "# ---------- 지표 ----------\n",
    "mae  = mean_absolute_error(trues_inv.ravel(), preds_inv.ravel())\n",
    "rmse = np.sqrt(mean_squared_error(trues_inv.ravel(), preds_inv.ravel()))\n",
    "r2   = r2_score(trues_inv.ravel(), preds_inv.ravel())\n",
    "print(f\"\\n전체 MAE {mae:.3f}  RMSE {rmse:.3f}  R² {r2:.3f}\")\n",
    "\n",
    "# ---------- 시계열 long-form ----------\n",
    "Tte = times_te if 'times_te' in globals() else Tte\n",
    "rows = []\n",
    "for i in range(trues_inv.shape[0]):\n",
    "    base_t = pd.Timestamp(Tte[i])\n",
    "    for h in range(HORIZON):\n",
    "        rows.append([\n",
    "            base_t + pd.Timedelta(hours=h+1),\n",
    "            trues_inv[i, h],\n",
    "            preds_inv[i, h],\n",
    "            i, h+1\n",
    "        ])\n",
    "df_long = pd.DataFrame(rows, columns=[\"Time\",\"True\",\"Pred\",\"win_idx\",\"h\"])\n",
    "df_long = df_long.sort_values(\"Time\").reset_index(drop=True)\n",
    "\n",
    "# ---------- 이상치 탐지 ----------\n",
    "df_long[\"abs_err\"] = (df_long[\"True\"] - df_long[\"Pred\"]).abs()\n",
    "thr = df_long[\"abs_err\"].quantile(1.0 - OUTLIER_FRAC)\n",
    "df_long[\"is_outlier\"] = df_long[\"abs_err\"] >= thr\n",
    "print(f\"\\n[INFO] 상위 {int(OUTLIER_FRAC*100)}% 임계치: {thr:.3f}, 이상치 수: {df_long['is_outlier'].sum()}\")\n",
    "\n",
    "# ---------- 전체 플롯 ----------\n",
    "sys = platform.system()\n",
    "if sys == \"Windows\": plt.rcParams[\"font.family\"] = \"Malgun Gothic\"\n",
    "elif sys == \"Darwin\": plt.rcParams[\"font.family\"] = \"AppleGothic\"\n",
    "else: plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "plt.figure(figsize=(22,5))\n",
    "plt.title(f\"전체 결과 (Top {int(OUTLIER_FRAC*100)}% 이상치 강조)\")\n",
    "plt.plot(df_long[\"Time\"], df_long[\"True\"], color=\"#2b83ba\", linewidth=1.0, label=\"True\")\n",
    "plt.plot(df_long[\"Time\"], df_long[\"Pred\"], color=\"#fdae61\", linewidth=1.0, label=\"Pred\")\n",
    "out = df_long[df_long[\"is_outlier\"]]\n",
    "plt.scatter(out[\"Time\"], out[\"True\"], s=14, color=\"crimson\", label=\"Top 1% Outliers\", zorder=3)\n",
    "plt.xlabel(\"Time\"); plt.ylabel(\"Value\"); plt.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"plot_all_top1.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# ---------- 주간 확대 플롯 ----------\n",
    "def plot_week_window(df_long, mode=\"calendar\", week_key=None, week_rule=\"W-MON\"):\n",
    "    dfl = df_long.copy()\n",
    "    dfl[\"Time\"] = pd.to_datetime(dfl[\"Time\"])\n",
    "    dfl = dfl.sort_values(\"Time\").reset_index(drop=True)\n",
    "\n",
    "    if mode == \"calendar\":\n",
    "        dfl[\"week_period\"] = dfl[\"Time\"].dt.to_period(week_rule)\n",
    "        if week_key is None:\n",
    "            week_sel = dfl[\"week_period\"].iloc[0]\n",
    "        else:\n",
    "            wk = pd.to_datetime(week_key)\n",
    "            week_sel = wk.to_period(week_rule)\n",
    "        week_df = dfl[dfl[\"week_period\"] == week_sel]\n",
    "        title = f\"주간 확대 (Calendar {week_rule}): {week_sel.start_time.date()} ~ {week_sel.end_time.date()}\"\n",
    "\n",
    "    elif mode == \"rolling\":\n",
    "        start = pd.to_datetime(week_key)\n",
    "        end = start + pd.Timedelta(days=7)\n",
    "        week_df = dfl[(dfl[\"Time\"] >= start) & (dfl[\"Time\"] < end)]\n",
    "        title = f\"주간 확대 (Rolling): {start.date()} ~ {(end - pd.Timedelta(days=1)).date()}\"\n",
    "\n",
    "    else:  # index mode\n",
    "        t0 = dfl[\"Time\"].min().normalize()\n",
    "        dfl[\"day_idx\"] = ((dfl[\"Time\"] - t0).dt.total_seconds() // (3600*24)).astype(int)\n",
    "        dfl[\"week_idx\"] = (dfl[\"day_idx\"] // 7).astype(int)\n",
    "        week_df = dfl[dfl[\"week_idx\"] == int(week_key)]\n",
    "        title = f\"주간 확대 (Week {int(week_key)+1}): {week_df['Time'].min().date()} ~ {week_df['Time'].max().date()}\"\n",
    "\n",
    "    if week_df.empty:\n",
    "        print(f\"[WARN] {mode} 기준 데이터 없음. 다른 주 선택.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(18,4))\n",
    "    plt.title(title)\n",
    "    plt.plot(week_df[\"Time\"], week_df[\"True\"], color=\"#2b83ba\", linewidth=1.7, label=\"True\")\n",
    "    plt.plot(week_df[\"Time\"], week_df[\"Pred\"], color=\"#fdae61\", linewidth=1.7, label=\"Pred\")\n",
    "    out_w = week_df[week_df[\"is_outlier\"]]\n",
    "    if not out_w.empty:\n",
    "        plt.scatter(out_w[\"Time\"], out_w[\"True\"], s=18, color=\"crimson\", label=\"Outlier\", zorder=3)\n",
    "    plt.xlabel(\"Time\"); plt.ylabel(\"Value\"); plt.legend(loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"plot_week_{mode}2.png\"), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# 실행\n",
    "plot_week_window(df_long, mode=WEEK_MODE, week_key=WEEK_KEY, week_rule=WEEK_RULE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9299d04",
   "metadata": {},
   "source": [
    "# ✅ 결론 요약\n",
    "\n",
    "+ ✅ 좋은 점\n",
    "    + 일주기 패턴 완벽하게 학습 (낮-밤 주기 잘 맞음)\n",
    "    + 야간~저출력 구간에서는 오차 거의 없음\n",
    "    + 평균 오차 (MAE 511, RMSE 2600, R² 0.71)는 실제 발전량 데이터 기준으로는 상당히 양호한 수준\n",
    "\n",
    "+ ⚠️ 한계\n",
    "    + 피크(정오~오후) 시간대의 예측 변동을 세밀하게 반영하지 못함\n",
    "    + 극값(최대 발전량) 부근에서 일사량·구름 변화 등 외부 요인이 반영되지 않음\n",
    "    + 즉, 정확도 상한선이 데이터 구성과 모델 구조에 의해 결정된 상태"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25244c66",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
