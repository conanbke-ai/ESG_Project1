{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3eaeaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# =========================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# ‚úÖ CNN-LSTM + Optuna + SHAP ÌûàÌä∏Îßµ + Ïù¥ÏÉÅÏπò Ï†úÍ±∞ + ÏïôÏÉÅÎ∏î\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# =========================================================\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mgc\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mshap\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mplatform\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# ‚úÖ CNN-LSTM + Optuna + SHAP ÌûàÌä∏Îßµ + Ïù¥ÏÉÅÏπò Ï†úÍ±∞ + ÏïôÏÉÅÎ∏î\n",
    "# =========================================================\n",
    "import os, sys, shap, torch, optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import zscore\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------\n",
    "# Î°úÍπÖ\n",
    "# -------------------------\n",
    "sys.path.append(r\"C:\\ESG_Project1\\util\")\n",
    "from logger import setup_logger\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "# -------------------------\n",
    "# ÌôòÍ≤Ω ÏÑ§Ï†ï\n",
    "# -------------------------\n",
    "TRAIN_CSV = r\"C:\\ESG_Project1\\file\\merge_data\\train_data.csv\"\n",
    "TEST_CSV  = r\"C:\\ESG_Project1\\file\\merge_data\\test_data.csv\"\n",
    "COL_Y     = \"Ìï©ÏÇ∞Î∞úÏ†ÑÎüâ(MWh)\"\n",
    "COL_TIME  = \"ÏùºÏãú\"\n",
    "COL_PLANT = \"Î∞úÏ†ÑÍµ¨Î∂Ñ\"\n",
    "NUM_FEATS = [\"Í∏∞Ïò®(¬∞C)\", \"Í∞ïÏàòÎüâ(mm)\", \"ÏùºÏ°∞(hr)\", \"ÏùºÏÇ¨(MJ/m2)\"]\n",
    "SEQ_LEN, HORIZON = 168, 24\n",
    "OUTLIER_FRAC = 0.01\n",
    "SAVE_DIR = r\"C:\\ESG_Project1\\cnn_lstm\\output\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# =========================================================\n",
    "# üß© Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ + Ïù¥ÏÉÅÏπò Ï†úÍ±∞\n",
    "# =========================================================\n",
    "def read_csv_auto(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "    except:\n",
    "        df = pd.read_csv(path, encoding=\"cp949\")\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    for col in NUM_FEATS + [COL_Y]:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.dropna(subset=NUM_FEATS + [COL_Y])\n",
    "    df = df[np.abs(zscore(df[COL_Y])) < 3]  # Ïù¥ÏÉÅÏπò Ï†úÍ±∞\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "train_df = preprocess_data(read_csv_auto(TRAIN_CSV))\n",
    "test_df  = preprocess_data(read_csv_auto(TEST_CSV))\n",
    "logger.info(f\"Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# =========================================================\n",
    "# üîß Feature Scaling\n",
    "# =========================================================\n",
    "scaler = MinMaxScaler()\n",
    "Xtr = scaler.fit_transform(train_df[NUM_FEATS])\n",
    "Xte = scaler.transform(test_df[NUM_FEATS])\n",
    "ytr = np.log1p(train_df[COL_Y].values)\n",
    "yte = np.log1p(test_df[COL_Y].values)\n",
    "MAX_LOG_Y = float(np.log1p(train_df[COL_Y].quantile(0.999)))\n",
    "\n",
    "# =========================================================\n",
    "# üß† CNN-LSTM Î™®Îç∏ Ï†ïÏùò\n",
    "# =========================================================\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, n_features, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(n_features, hidden_size, 3, padding=1)\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, HORIZON)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.relu(self.bn(self.conv(x)))\n",
    "        x = x.transpose(1, 2)\n",
    "        out, _ = self.lstm(x)\n",
    "        h = self.drop(self.relu(self.fc1(out[:, -1, :])))\n",
    "        y = torch.nn.functional.softplus(self.fc2(h))\n",
    "        return torch.clamp(y, max=MAX_LOG_Y)\n",
    "\n",
    "# =========================================================\n",
    "# üì¶ Dataset\n",
    "# =========================================================\n",
    "class WindowedDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len=SEQ_LEN):\n",
    "        self.X, self.y, self.seq_len = X, y, seq_len\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len - HORIZON + 1\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.X[idx:idx+self.seq_len]).float(),\n",
    "            torch.tensor(self.y[idx+self.seq_len:idx+self.seq_len+HORIZON]).float()\n",
    "        )\n",
    "\n",
    "# =========================================================\n",
    "# ‚öôÔ∏è Optuna ÌÉêÏÉâ\n",
    "# =========================================================\n",
    "def objective(trial):\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 64, 256)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.4)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    model = CNNLSTM(Xtr.shape[1], hidden_size, num_layers, dropout).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "    ds = WindowedDataset(Xtr, ytr)\n",
    "    loader = DataLoader(ds, batch_size=128, shuffle=True)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.tensor(Xte[:500]).unsqueeze(1).float().to(DEVICE)\n",
    "        pred = model(X_tensor).cpu().numpy().ravel()\n",
    "        y_true = yte[:len(pred)]\n",
    "        return mean_squared_error(np.expm1(y_true), np.expm1(pred))\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "best_params = study.best_params\n",
    "logger.info(f\"Best Params: {best_params}\")\n",
    "\n",
    "# =========================================================\n",
    "# üß© Î™®Îç∏ ÌïôÏäµ (ÏïôÏÉÅÎ∏î)\n",
    "# =========================================================\n",
    "models = []\n",
    "for i in range(3):\n",
    "    model = CNNLSTM(Xtr.shape[1], **best_params).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "    ds = WindowedDataset(Xtr, ytr)\n",
    "    loader = DataLoader(ds, batch_size=128, shuffle=True)\n",
    "\n",
    "    for ep in range(50):\n",
    "        model.train()\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    models.append(model)\n",
    "\n",
    "# =========================================================\n",
    "# üîç ÏòàÏ∏° + ÏÑ±Îä•\n",
    "# =========================================================\n",
    "def predict(model, X):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X)-SEQ_LEN-HORIZON, HORIZON):\n",
    "            xb = torch.tensor(X[i:i+SEQ_LEN]).unsqueeze(0).float().to(DEVICE)\n",
    "            preds.append(model(xb).cpu().numpy().ravel())\n",
    "    return np.concatenate(preds)\n",
    "\n",
    "preds_list = [predict(m, Xte) for m in models]\n",
    "preds_mean = np.mean(preds_list, axis=0)\n",
    "y_true = np.expm1(yte[:len(preds_mean)])\n",
    "pred_inv = np.expm1(preds_mean)\n",
    "\n",
    "mae = mean_absolute_error(y_true, pred_inv)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, pred_inv))\n",
    "r2 = r2_score(y_true, pred_inv)\n",
    "logger.info(f\"Final Ensemble ‚Üí MAE={mae:.3f} | RMSE={rmse:.3f} | R¬≤={r2:.4f}\")\n",
    "\n",
    "# =========================================================\n",
    "# üß† SHAP ÌûàÌä∏Îßµ (ÏãúÍ∞Ñ √ó ÌäπÏÑ±)\n",
    "# =========================================================\n",
    "background = torch.tensor(Xtr[:200]).unsqueeze(1).float().to(DEVICE)\n",
    "test_sample = torch.tensor(Xte[:200]).unsqueeze(1).float().to(DEVICE)\n",
    "explainer = shap.DeepExplainer(models[0], background)\n",
    "shap_values = explainer.shap_values(test_sample)[0]\n",
    "\n",
    "shap_df = pd.DataFrame(np.mean(np.abs(shap_values), axis=1), columns=NUM_FEATS)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(shap_df.T, cmap=\"RdYlBu_r\", annot=False)\n",
    "plt.title(\"SHAP Feature Importance (Time √ó Feature)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"shap_heatmap.png\"), dpi=200)\n",
    "plt.close()\n",
    "\n",
    "logger.info(f\"‚úÖ SHAP ÌûàÌä∏Îßµ Ï†ÄÏû• ÏôÑÎ£å: {os.path.join(SAVE_DIR, 'shap_heatmap.png')}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Jupyter)",
   "language": "python",
   "name": "py311_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
