{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3eaeaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m원격 Jupyter 서버 'http://localhost:8889/'에 연결하지 못했습니다. 서버가 실행 중이고 연결할 수 있는지 확인합니다.(원격 Jupyter 서버 'http://localhost:8889/'에 연결하지 못했습니다. 서버가 실행 중이고 연결할 수 있는지 확인합니다.(request to http://localhost:8889/api/kernels?1761604156996 failed, reason: ).)."
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# ✅ CNN-LSTM + Optuna + SHAP 히트맵 + 이상치 제거 + 앙상블\n",
    "# =========================================================\n",
    "import os, sys, gc, shap, torch, optuna, platform, logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import zscore\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------\n",
    "# 로깅\n",
    "# -------------------------\n",
    "sys.path.append(r\"C:\\ESG_Project1\\util\")\n",
    "from logger import setup_logger\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "# -------------------------\n",
    "# 환경 설정\n",
    "# -------------------------\n",
    "TRAIN_CSV = r\"C:\\ESG_Project1\\file\\merge_data\\train_data.csv\"\n",
    "TEST_CSV  = r\"C:\\ESG_Project1\\file\\merge_data\\test_data.csv\"\n",
    "COL_Y     = \"합산발전량(MWh)\"\n",
    "COL_TIME  = \"일시\"\n",
    "COL_PLANT = \"발전구분\"\n",
    "NUM_FEATS = [\"기온(°C)\", \"강수량(mm)\", \"일조(hr)\", \"일사(MJ/m2)\"]\n",
    "SEQ_LEN, HORIZON = 168, 24\n",
    "OUTLIER_FRAC = 0.01\n",
    "SAVE_DIR = r\"C:\\ESG_Project1\\cnn_lstm\\output\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# =========================================================\n",
    "# 🧩 데이터 전처리 + 이상치 제거\n",
    "# =========================================================\n",
    "def read_csv_auto(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "    except:\n",
    "        df = pd.read_csv(path, encoding=\"cp949\")\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    for col in NUM_FEATS + [COL_Y]:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.dropna(subset=NUM_FEATS + [COL_Y])\n",
    "    df = df[np.abs(zscore(df[COL_Y])) < 3]  # 이상치 제거\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "train_df = preprocess_data(read_csv_auto(TRAIN_CSV))\n",
    "test_df  = preprocess_data(read_csv_auto(TEST_CSV))\n",
    "logger.info(f\"Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# =========================================================\n",
    "# 🔧 Feature Scaling\n",
    "# =========================================================\n",
    "scaler = MinMaxScaler()\n",
    "Xtr = scaler.fit_transform(train_df[NUM_FEATS])\n",
    "Xte = scaler.transform(test_df[NUM_FEATS])\n",
    "ytr = np.log1p(train_df[COL_Y].values)\n",
    "yte = np.log1p(test_df[COL_Y].values)\n",
    "MAX_LOG_Y = float(np.log1p(train_df[COL_Y].quantile(0.999)))\n",
    "\n",
    "# =========================================================\n",
    "# 🧠 CNN-LSTM 모델 정의\n",
    "# =========================================================\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, n_features, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(n_features, hidden_size, 3, padding=1)\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, HORIZON)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.relu(self.bn(self.conv(x)))\n",
    "        x = x.transpose(1, 2)\n",
    "        out, _ = self.lstm(x)\n",
    "        h = self.drop(self.relu(self.fc1(out[:, -1, :])))\n",
    "        y = torch.nn.functional.softplus(self.fc2(h))\n",
    "        return torch.clamp(y, max=MAX_LOG_Y)\n",
    "\n",
    "# =========================================================\n",
    "# 📦 Dataset\n",
    "# =========================================================\n",
    "class WindowedDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len=SEQ_LEN):\n",
    "        self.X, self.y, self.seq_len = X, y, seq_len\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len - HORIZON + 1\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.X[idx:idx+self.seq_len]).float(),\n",
    "            torch.tensor(self.y[idx+self.seq_len:idx+self.seq_len+HORIZON]).float()\n",
    "        )\n",
    "\n",
    "# =========================================================\n",
    "# ⚙️ Optuna 탐색\n",
    "# =========================================================\n",
    "def objective(trial):\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 64, 256)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.4)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    model = CNNLSTM(Xtr.shape[1], hidden_size, num_layers, dropout).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "    ds = WindowedDataset(Xtr, ytr)\n",
    "    loader = DataLoader(ds, batch_size=128, shuffle=True)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.tensor(Xte[:500]).unsqueeze(1).float().to(DEVICE)\n",
    "        pred = model(X_tensor).cpu().numpy().ravel()\n",
    "        y_true = yte[:len(pred)]\n",
    "        return mean_squared_error(np.expm1(y_true), np.expm1(pred))\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "best_params = study.best_params\n",
    "logger.info(f\"Best Params: {best_params}\")\n",
    "\n",
    "# =========================================================\n",
    "# 🧩 모델 학습 (앙상블)\n",
    "# =========================================================\n",
    "models = []\n",
    "for i in range(3):\n",
    "    model = CNNLSTM(Xtr.shape[1], **best_params).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "    ds = WindowedDataset(Xtr, ytr)\n",
    "    loader = DataLoader(ds, batch_size=128, shuffle=True)\n",
    "\n",
    "    for ep in range(50):\n",
    "        model.train()\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    models.append(model)\n",
    "\n",
    "# =========================================================\n",
    "# 🔍 예측 + 성능\n",
    "# =========================================================\n",
    "def predict(model, X):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X)-SEQ_LEN-HORIZON, HORIZON):\n",
    "            xb = torch.tensor(X[i:i+SEQ_LEN]).unsqueeze(0).float().to(DEVICE)\n",
    "            preds.append(model(xb).cpu().numpy().ravel())\n",
    "    return np.concatenate(preds)\n",
    "\n",
    "preds_list = [predict(m, Xte) for m in models]\n",
    "preds_mean = np.mean(preds_list, axis=0)\n",
    "y_true = np.expm1(yte[:len(preds_mean)])\n",
    "pred_inv = np.expm1(preds_mean)\n",
    "\n",
    "mae = mean_absolute_error(y_true, pred_inv)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, pred_inv))\n",
    "r2 = r2_score(y_true, pred_inv)\n",
    "logger.info(f\"Final Ensemble → MAE={mae:.3f} | RMSE={rmse:.3f} | R²={r2:.4f}\")\n",
    "\n",
    "# =========================================================\n",
    "# 🧠 SHAP 히트맵 (시간 × 특성)\n",
    "# =========================================================\n",
    "background = torch.tensor(Xtr[:200]).unsqueeze(1).float().to(DEVICE)\n",
    "test_sample = torch.tensor(Xte[:200]).unsqueeze(1).float().to(DEVICE)\n",
    "explainer = shap.DeepExplainer(models[0], background)\n",
    "shap_values = explainer.shap_values(test_sample)[0]\n",
    "\n",
    "shap_df = pd.DataFrame(np.mean(np.abs(shap_values), axis=1), columns=NUM_FEATS)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(shap_df.T, cmap=\"RdYlBu_r\", annot=False)\n",
    "plt.title(\"SHAP Feature Importance (Time × Feature)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"shap_heatmap.png\"), dpi=200)\n",
    "plt.close()\n",
    "\n",
    "logger.info(f\"✅ SHAP 히트맵 저장 완료: {os.path.join(SAVE_DIR, 'shap_heatmap.png')}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python311]",
   "language": "python",
   "name": "conda-env-python311-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
