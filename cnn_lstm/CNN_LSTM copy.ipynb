{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959f43e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 통합 학습 + 테스트 + Optuna 최적화 + 시각화 모듈\n",
    "# -------------------------\n",
    "import os, sys, random, numpy as np, pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "\n",
    "# -------------------------\n",
    "# 한글 폰트 설정\n",
    "# -------------------------\n",
    "def setup_font():\n",
    "    font_path = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "    if os.path.exists(font_path):\n",
    "        font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
    "        rc('font', family=font_name)\n",
    "    else:\n",
    "        rc('font', family='AppleGothic')  # MacOS 예시\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# -------------------------\n",
    "# 재현성 설정\n",
    "# -------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Seq2Seq CNN-LSTM 모델 정의\n",
    "# -------------------------\n",
    "class Seq2SeqCNNLSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, conv_channels=(32,16), lstm_hidden=64, output_steps=24, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, conv_channels[0], 3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm1d(conv_channels[0])\n",
    "        self.conv2 = nn.Conv1d(conv_channels[0], conv_channels[1], 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm1d(conv_channels[1])\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoder_lstm = nn.LSTM(conv_channels[1], lstm_hidden, batch_first=True)\n",
    "        self.decoder_lstm = nn.LSTM(1, lstm_hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden,1)\n",
    "        self.output_steps = output_steps\n",
    "\n",
    "    def forward(self, x, y=None, teacher_forcing_ratio=0.0):\n",
    "        x = self.relu(self.bn1(self.conv1(x.transpose(1,2))))\n",
    "        x = self.relu(self.bn2(self.conv2(x))).transpose(1,2)\n",
    "        _, (hidden, cell) = self.encoder_lstm(x)\n",
    "        decoder_input = x[:,-1,0].unsqueeze(-1)\n",
    "        outputs = []\n",
    "        for t in range(self.output_steps):\n",
    "            decoder_output, (hidden, cell) = self.decoder_lstm(decoder_input.unsqueeze(1), (hidden,cell))\n",
    "            out = self.fc(decoder_output).squeeze(1)\n",
    "            outputs.append(out)\n",
    "            decoder_input = y[:,t] if (y is not None and torch.rand(1).item() < teacher_forcing_ratio) else out\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "# -------------------------\n",
    "# 시퀀스 생성\n",
    "# -------------------------\n",
    "def create_sequences(data, input_steps=168, output_steps=24):\n",
    "    total_steps = len(data) - input_steps - output_steps + 1\n",
    "    X = np.array([data[i:i+input_steps] for i in range(total_steps)])\n",
    "    y = np.array([data[i+input_steps:i+input_steps+output_steps] for i in range(total_steps)])\n",
    "    return X, y\n",
    "\n",
    "# -------------------------\n",
    "# Optuna objective\n",
    "# -------------------------\n",
    "def objective(trial, train_loader, val_loader, input_steps=168, output_steps=24, device=\"cpu\"):\n",
    "    conv1_ch = trial.suggest_int(\"conv1_ch\", 16, 48, step=16)\n",
    "    conv2_ch = trial.suggest_int(\"conv2_ch\", 8, 32, step=8)\n",
    "    lstm_hidden = trial.suggest_int(\"lstm_hidden\", 32, 128, step=32)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5, step=0.1)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    model = Seq2SeqCNNLSTM(conv_channels=(conv1_ch, conv2_ch), lstm_hidden=lstm_hidden,\n",
    "                           output_steps=output_steps, dropout=dropout).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler_amp = torch.amp.GradScaler()\n",
    "\n",
    "    best_val = np.inf\n",
    "    no_improve = 0\n",
    "    epochs_trial = 10\n",
    "    early_patience_trial = 3\n",
    "\n",
    "    for epoch in range(epochs_trial):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast(device_type=device):\n",
    "                y_pred = model(xb, yb, teacher_forcing_ratio=0.5)\n",
    "                loss = criterion(y_pred, yb)\n",
    "            scaler_amp.scale(loss).backward()\n",
    "            scaler_amp.step(optimizer)\n",
    "            scaler_amp.update()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                with torch.amp.autocast(device_type=device):\n",
    "                    val_loss += criterion(model(xb), yb).item()*xb.size(0)\n",
    "        avg_val = val_loss / len(val_loader.dataset)\n",
    "\n",
    "        if avg_val < best_val - 1e-6:\n",
    "            best_val = avg_val\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= early_patience_trial:\n",
    "                break\n",
    "    return best_val\n",
    "\n",
    "# -------------------------\n",
    "# 전체 파이프라인\n",
    "# -------------------------\n",
    "def run_pipeline(train_csv, test_csv, target_col='합산발전량(MWh)', input_steps=168, output_steps=24,\n",
    "                 batch_size=128, epochs=120, n_trials=10, lr=1e-3, output_dir='output'):\n",
    "\n",
    "    setup_font()\n",
    "    set_seed()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"💻 사용 디바이스: {device}\")\n",
    "\n",
    "    OUTPUT_DIR = Path(output_dir) / datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_df = pd.read_csv(train_csv, index_col=0, parse_dates=True)\n",
    "    test_df  = pd.read_csv(test_csv, index_col=0, parse_dates=True)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = scaler.fit_transform(train_df[[target_col]].values)\n",
    "    test_scaled  = scaler.transform(test_df[[target_col]].values)\n",
    "\n",
    "    X_all, y_all = create_sequences(train_scaled, input_steps, output_steps)\n",
    "    X_tensor = torch.tensor(X_all, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_all, dtype=torch.float32)\n",
    "\n",
    "    val_ratio = 0.1\n",
    "    val_size = int(len(X_tensor)*val_ratio)\n",
    "    train_size = len(X_tensor)-val_size\n",
    "    train_dataset, val_dataset = random_split(TensorDataset(X_tensor, y_tensor), [train_size,val_size])\n",
    "\n",
    "    num_workers = 0 if os.name=='nt' else 4\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # -------------------------\n",
    "    # Optuna 최적화\n",
    "    # -------------------------\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial: objective(trial, train_loader, val_loader, input_steps, output_steps, device),\n",
    "                   n_trials=n_trials, show_progress_bar=True)\n",
    "    best_params = study.best_params\n",
    "    print(f\"🏆 최적 하이퍼파라미터: {best_params}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 최종 모델 학습\n",
    "    # -------------------------\n",
    "    model = Seq2SeqCNNLSTM(conv_channels=(best_params[\"conv1_ch\"], best_params[\"conv2_ch\"]),\n",
    "                           lstm_hidden=best_params[\"lstm_hidden\"],\n",
    "                           output_steps=output_steps,\n",
    "                           dropout=best_params[\"dropout\"]).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "    scaler_amp = torch.amp.GradScaler()\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    no_improve = 0\n",
    "    early_patience = 15\n",
    "    history = {\"train_loss\":[], \"val_loss\":[], \"val_rmse\":[]}\n",
    "    checkpoint_path = OUTPUT_DIR / \"best_model.pt\"\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        teacher_ratio = max(0.3, 0.7-0.4*(epoch-1)/epochs)\n",
    "        for xb,yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast(device_type=device):\n",
    "                y_pred = model(xb,yb,teacher_forcing_ratio=teacher_ratio)\n",
    "                loss = criterion(y_pred,yb)\n",
    "            scaler_amp.scale(loss).backward()\n",
    "            scaler_amp.step(optimizer)\n",
    "            scaler_amp.update()\n",
    "            train_loss += loss.item()*xb.size(0)\n",
    "        avg_train = train_loss/len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss=0\n",
    "        val_preds,val_trues=[],[]\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in val_loader:\n",
    "                xb,yb = xb.to(device), yb.to(device)\n",
    "                with torch.amp.autocast(device_type=device):\n",
    "                    y_pred = model(xb)\n",
    "                val_loss += criterion(y_pred,yb).item()*xb.size(0)\n",
    "                val_preds.append(y_pred.cpu().numpy())\n",
    "                val_trues.append(yb.cpu().numpy())\n",
    "        avg_val = val_loss/len(val_loader.dataset)\n",
    "        val_preds = np.concatenate(val_preds,axis=0).reshape(-1,1)\n",
    "        val_trues = np.concatenate(val_trues,axis=0).reshape(-1,1)\n",
    "        val_rmse = np.sqrt(mean_squared_error(val_trues,val_preds))\n",
    "\n",
    "        history[\"train_loss\"].append(avg_train)\n",
    "        history[\"val_loss\"].append(avg_val)\n",
    "        history[\"val_rmse\"].append(val_rmse)\n",
    "\n",
    "        if avg_val < best_val_loss-1e-6:\n",
    "            best_val_loss = avg_val\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= early_patience:\n",
    "                print(f\"✅ Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} | Train Loss:{avg_train:.6f} | Val Loss:{avg_val:.6f} | Val RMSE:{val_rmse:.2f} | TF:{teacher_ratio:.2f}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 테스트 예측 + 분석\n",
    "    # -------------------------\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    model.eval()\n",
    "    rolling_input = train_scaled[-input_steps:].tolist()\n",
    "    predicted_scaled = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        i=0\n",
    "        while i<len(test_scaled):\n",
    "            steps_remaining = len(test_scaled)-i\n",
    "            steps_to_predict = min(output_steps,steps_remaining)\n",
    "            X_input = torch.tensor(rolling_input[-input_steps:],dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            y_pred = model(X_input,teacher_forcing_ratio=0.0).cpu().numpy().flatten()\n",
    "            for step in range(steps_to_predict):\n",
    "                rolling_input.append([y_pred[step]])\n",
    "                predicted_scaled.append(y_pred[step])\n",
    "            i += steps_to_predict\n",
    "\n",
    "    predicted_scaled = np.array(predicted_scaled).reshape(-1,1)\n",
    "    predicted_generation = scaler.inverse_transform(predicted_scaled)\n",
    "    y_true = test_df[[target_col]].values\n",
    "    rmse = np.sqrt(mean_squared_error(y_true,predicted_generation))\n",
    "    r2   = r2_score(y_true,predicted_generation)\n",
    "    print(f\"테스트셋 평가 결과: RMSE={rmse:.2f}, R²={r2:.4f}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 시각화 + CSV 저장\n",
    "    # -------------------------\n",
    "    train_min, train_max = train_df[target_col].min(), train_df[target_col].max()\n",
    "    test_actual = y_true.flatten()\n",
    "    test_pred_flat = predicted_generation.flatten()\n",
    "    out_of_range_mask = (test_actual<train_min) | (test_actual>train_max)\n",
    "    in_range_mask = ~out_of_range_mask\n",
    "\n",
    "    # 범위 안/밖 RMSE/R²\n",
    "    y_true_in  = test_actual[in_range_mask]\n",
    "    y_pred_in  = test_pred_flat[in_range_mask]\n",
    "    rmse_in    = np.sqrt(mean_squared_error(y_true_in, y_pred_in))\n",
    "    r2_in      = r2_score(y_true_in, y_pred_in)\n",
    "\n",
    "    y_true_out = test_actual[out_of_range_mask]\n",
    "    y_pred_out = test_pred_flat[out_of_range_mask]\n",
    "    rmse_out   = np.sqrt(mean_squared_error(y_true_out,y_pred_out)) if len(y_true_out)>0 else np.nan\n",
    "    r2_out     = r2_score(y_true_out,y_pred_out) if len(y_true_out)>0 else np.nan\n",
    "    out_ratio  = len(y_true_out)/len(test_actual)*100\n",
    "    print(f\"학습 범위 안: RMSE={rmse_in:.2f}, R²={r2_in:.4f}, 범위 밖 비율={out_ratio:.2f}%\")\n",
    "\n",
    "    # CSV 저장\n",
    "    result_path = OUTPUT_DIR / \"predicted_generation.csv\"\n",
    "    pd.DataFrame({\n",
    "        \"날짜\": test_df.index,\n",
    "        \"실제발전량(MWh)\": test_actual,\n",
    "        \"예측발전량(MWh)\": test_pred_flat,\n",
    "        \"오차(MWh)\": test_pred_flat-test_actual\n",
    "    }).to_csv(result_path,index=False)\n",
    "    print(f\"✅ 테스트셋 예측 CSV 저장 완료: {result_path}\")\n",
    "\n",
    "    if len(y_true_out)>0:\n",
    "        out_csv_path = OUTPUT_DIR / \"out_of_range_generation.csv\"\n",
    "        pd.DataFrame({\n",
    "            \"날짜\": test_df.index[out_of_range_mask],\n",
    "            \"실제발전량(MWh)\": y_true_out,\n",
    "            \"예측발전량(MWh)\": y_pred_out,\n",
    "            \"오차(MWh)\": y_pred_out-y_true_out\n",
    "        }).to_csv(out_csv_path,index=False)\n",
    "        print(f\"✅ 학습 범위 벗어난 값 CSV 저장 완료: {out_csv_path}\")\n",
    "\n",
    "    # 통합 시각화\n",
    "    plt.figure(figsize=(16,7))\n",
    "    plt.plot(test_df.index,test_actual,label=\"실제발전량\",color=\"blue\")\n",
    "    plt.plot(test_df.index,test_pred_flat,label=\"예측발전량\",color=\"red\",alpha=0.7)\n",
    "    plt.fill_between(test_df.index, train_min, train_max, color='yellow', alpha=0.2, label=\"학습 범위\")\n",
    "    if len(y_true_out)>0:\n",
    "        plt.scatter(test_df.index[out_of_range_mask], y_true_out, color='black', label=\"학습 범위 밖 값\", zorder=5)\n",
    "    plt.title(f\"테스트셋 예측 | RMSE={rmse:.2f}, R²={r2:.4f}\")\n",
    "    plt.xlabel(\"날짜\")\n",
    "    plt.ylabel(\"발전량 (MWh)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    graph_path = OUTPUT_DIR / \"testset_analysis.png\"\n",
    "    plt.savefig(graph_path,dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"✅ 통합 분석 그래프 저장 완료: {graph_path}\")\n",
    "\n",
    "    return model, history, predicted_generation, OUTPUT_DIR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "311.venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
