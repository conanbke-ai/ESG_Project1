{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af3eaeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CNN-LSTM í†µí•© íŒŒì´í”„ë¼ì¸ + SHAP ì‹œê°í™” (ì‹œê°„ë³„/íŠ¹ì§•ë³„ íˆíŠ¸ë§µ)\n",
    "# ==========================================\n",
    "import os, sys, random, numpy as np, pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "import shap\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------\n",
    "# ë¡œê¹… & í°íŠ¸ ì„¤ì •\n",
    "# -------------------------\n",
    "sys.path.append(r\"C:\\ESG_Project1\\util\")\n",
    "from logger import setup_logger\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "def setup_font():\n",
    "    font_path = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "    if os.path.exists(font_path):\n",
    "        font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
    "        rc('font', family=font_name)\n",
    "    else:\n",
    "        rc('font', family='AppleGothic')  # MacOS ì˜ˆì‹œ\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# -------------------------\n",
    "# ì¬í˜„ì„±\n",
    "# -------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# CNN-LSTM ëª¨ë¸ ì •ì˜\n",
    "# -------------------------\n",
    "class Seq2SeqCNNLSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, conv_channels=(32,16), lstm_hidden=64, output_steps=24, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, conv_channels[0], 3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm1d(conv_channels[0])\n",
    "        self.conv2 = nn.Conv1d(conv_channels[0], conv_channels[1], 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm1d(conv_channels[1])\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoder_lstm = nn.LSTM(conv_channels[1], lstm_hidden, batch_first=True)\n",
    "        self.decoder_lstm = nn.LSTM(1, lstm_hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden,1)\n",
    "        self.output_steps = output_steps\n",
    "\n",
    "    def forward(self, x, y=None, teacher_forcing_ratio=0.0):\n",
    "        x = self.relu(self.bn1(self.conv1(x.transpose(1,2))))\n",
    "        x = self.relu(self.bn2(self.conv2(x))).transpose(1,2)\n",
    "        _, (hidden, cell) = self.encoder_lstm(x)\n",
    "        decoder_input = x[:,-1,0].unsqueeze(-1)\n",
    "        outputs = []\n",
    "        for t in range(self.output_steps):\n",
    "            decoder_output, (hidden, cell) = self.decoder_lstm(decoder_input.unsqueeze(1), (hidden,cell))\n",
    "            out = self.fc(decoder_output).squeeze(1)\n",
    "            outputs.append(out)\n",
    "            decoder_input = y[:,t] if (y is not None and torch.rand(1).item() < teacher_forcing_ratio) else out\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "# -------------------------\n",
    "# ì‹œí€€ìŠ¤ ìƒì„±\n",
    "# -------------------------\n",
    "def create_sequences(data, input_steps=168, output_steps=24):\n",
    "    total_steps = len(data) - input_steps - output_steps + 1\n",
    "    X = np.array([data[i:i+input_steps] for i in range(total_steps)])\n",
    "    y = np.array([data[i+input_steps:i+input_steps+output_steps] for i in range(total_steps)])\n",
    "    return X, y\n",
    "\n",
    "# -------------------------\n",
    "# Optuna objective\n",
    "# -------------------------\n",
    "def objective(trial, X_tensor, y_tensor, input_steps, output_steps, device):\n",
    "    sample_ratio = 0.2\n",
    "    total_len = len(X_tensor)\n",
    "    sample_size = int(total_len * sample_ratio)\n",
    "    indices = np.random.choice(total_len, sample_size, replace=False)\n",
    "    X_sample = X_tensor[indices]\n",
    "    y_sample = y_tensor[indices]\n",
    "\n",
    "    val_ratio = 0.1\n",
    "    val_size = int(sample_size*val_ratio)\n",
    "    train_size = sample_size - val_size\n",
    "    train_dataset, val_dataset = random_split(TensorDataset(X_sample, y_sample), [train_size,val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    conv1_ch = trial.suggest_int(\"conv1_ch\", 16, 48, step=16)\n",
    "    conv2_ch = trial.suggest_int(\"conv2_ch\", 8, 32, step=8)\n",
    "    lstm_hidden = trial.suggest_int(\"lstm_hidden\", 32, 128, step=32)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5, step=0.1)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    model = Seq2SeqCNNLSTM(conv_channels=(conv1_ch, conv2_ch), lstm_hidden=lstm_hidden,\n",
    "                           output_steps=output_steps, dropout=dropout).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val = np.inf\n",
    "    no_improve = 0\n",
    "    epochs_trial = 5\n",
    "    early_patience_trial = 2\n",
    "\n",
    "    for epoch in range(epochs_trial):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(xb, yb, teacher_forcing_ratio=0.5)\n",
    "            loss = criterion(y_pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                val_loss += criterion(model(xb), yb).item()*xb.size(0)\n",
    "        avg_val = val_loss / len(val_loader.dataset)\n",
    "\n",
    "        trial.report(avg_val, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if avg_val < best_val - 1e-6:\n",
    "            best_val = avg_val\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= early_patience_trial:\n",
    "                break\n",
    "    return best_val\n",
    "\n",
    "# -------------------------\n",
    "# íŒŒì´í”„ë¼ì¸ + SHAP íˆíŠ¸ë§µ\n",
    "# -------------------------\n",
    "def run_pipeline_shap_heatmap(train_csv, test_csv, target_col='í•©ì‚°ë°œì „ëŸ‰(MWh)',\n",
    "                              input_steps=168, output_steps=24,\n",
    "                              batch_size=128, epochs=120, n_trials=10,\n",
    "                              output_dir='output', shap_sample_size=500):\n",
    "\n",
    "    setup_font()\n",
    "    set_seed()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"ğŸ’» ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "    OUTPUT_DIR = Path(output_dir) / datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ë°ì´í„° ë¡œë”©\n",
    "    train_df = pd.read_csv(train_csv, index_col=0, parse_dates=True)\n",
    "    test_df  = pd.read_csv(test_csv, index_col=0, parse_dates=True)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = scaler.fit_transform(train_df[[target_col]].values)\n",
    "    test_scaled  = scaler.transform(test_df[[target_col]].values)\n",
    "\n",
    "    X_all, y_all = create_sequences(train_scaled, input_steps, output_steps)\n",
    "    X_tensor = torch.tensor(X_all, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_all, dtype=torch.float32)\n",
    "\n",
    "    # Optuna ìµœì í™”\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "    study.optimize(lambda trial: objective(trial, X_tensor, y_tensor, input_steps, output_steps, device), n_trials=n_trials)\n",
    "    best_params = study.best_params\n",
    "    logger.info(f\"ğŸ† ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_params}\")\n",
    "\n",
    "    # ìµœì¢… í•™ìŠµ\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    val_ratio = 0.1\n",
    "    val_size = int(len(dataset)*val_ratio)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size,val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = Seq2SeqCNNLSTM(conv_channels=(best_params[\"conv1_ch\"], best_params[\"conv2_ch\"]),\n",
    "                           lstm_hidden=best_params[\"lstm_hidden\"],\n",
    "                           output_steps=output_steps,\n",
    "                           dropout=best_params[\"dropout\"]).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    no_improve = 0\n",
    "    early_patience = 15\n",
    "    checkpoint_path = OUTPUT_DIR / \"best_model.pt\"\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(xb, yb, teacher_forcing_ratio=0.5)\n",
    "            loss = criterion(y_pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*xb.size(0)\n",
    "        avg_train = train_loss/len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss=0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                y_pred = model(xb)\n",
    "                val_loss += criterion(y_pred,yb).item()*xb.size(0)\n",
    "        avg_val = val_loss/len(val_loader.dataset)\n",
    "\n",
    "        if avg_val < best_val_loss-1e-6:\n",
    "            best_val_loss = avg_val\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= early_patience:\n",
    "                logger.info(f\"âœ… Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        logger.info(f\"Epoch {epoch}/{epochs} | Train Loss:{avg_train:.6f} | Val Loss:{avg_val:.6f}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # í•™ìŠµì…‹ í‰ê°€\n",
    "    # -------------------------\n",
    "    model.eval()\n",
    "    train_preds_scaled = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            y_pred = model(xb, teacher_forcing_ratio=0.0).cpu().numpy()\n",
    "            train_preds_scaled.append(y_pred)\n",
    "    train_preds_scaled = np.vstack(train_preds_scaled).reshape(-1,1)\n",
    "    train_true = scaler.inverse_transform(y_tensor.numpy().reshape(-1,1))\n",
    "    train_pred = scaler.inverse_transform(train_preds_scaled)\n",
    "\n",
    "    train_mae = mean_absolute_error(train_true, train_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(train_true, train_pred))\n",
    "    train_r2 = r2_score(train_true, train_pred)\n",
    "\n",
    "    logger.info(f\"[Train] MAE={train_mae:.4f} | RMSE={train_rmse:.4f} | R2={train_r2:.4f}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡\n",
    "    # -------------------------\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    model.eval()\n",
    "    rolling_input = train_scaled[-input_steps:].tolist()\n",
    "    predicted_scaled = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        i=0\n",
    "        while i<len(test_scaled):\n",
    "            steps_remaining = len(test_scaled)-i\n",
    "            steps_to_predict = min(output_steps,steps_remaining)\n",
    "            X_input = torch.tensor(rolling_input[-input_steps:],dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            y_pred = model(X_input,teacher_forcing_ratio=0.0).cpu().numpy().flatten()\n",
    "            for step in range(steps_to_predict):\n",
    "                rolling_input.append([y_pred[step]])\n",
    "                predicted_scaled.append(y_pred[step])\n",
    "            i += steps_to_predict\n",
    "\n",
    "    predicted_scaled = np.array(predicted_scaled).reshape(-1,1)\n",
    "    predicted_generation = scaler.inverse_transform(predicted_scaled)\n",
    "    y_true = test_df[[target_col]].values\n",
    "    test_mae = mean_absolute_error(y_true, predicted_generation)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_true,predicted_generation))\n",
    "    test_r2   = r2_score(y_true,predicted_generation)\n",
    "    logger.info(f\"[Test] MAE={test_mae:.4f} | RMSE={test_rmse:.4f} | R2={test_r2:.4f}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # SHAP ì˜í–¥ë„ ê³„ì‚° (ì‹œê°„ë³„)\n",
    "    # -------------------------\n",
    "    shap_sample = min(len(X_tensor), shap_sample_size)\n",
    "    X_shap = X_tensor[:shap_sample].to(device)\n",
    "    model.eval()\n",
    "    explainer = shap.DeepExplainer(model, X_shap)\n",
    "    shap_values = explainer.shap_values(X_shap)  # (ìƒ˜í”Œ, ì‹œê°„, 1)\n",
    "    shap_values_np = np.array(shap_values).squeeze()  # (ìƒ˜í”Œ, ì‹œê°„)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.heatmap(shap_values_np.T, cmap='viridis', cbar_kws={'label':'SHAP value'})\n",
    "    plt.xlabel(\"ìƒ˜í”Œ\")\n",
    "    plt.ylabel(\"ì‹œê°„ step\")\n",
    "    plt.title(\"ì‹œê°„ë³„ SHAP ì˜í–¥ë„ íˆíŠ¸ë§µ\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / \"shap_heatmap.png\")\n",
    "    plt.close()\n",
    "    logger.info(f\"âœ… SHAP íˆíŠ¸ë§µ ì €ì¥ ì™„ë£Œ: {OUTPUT_DIR / 'shap_heatmap.png'}\")\n",
    "\n",
    "    return model, predicted_generation, OUTPUT_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-27 12:03:00,913]âœ… INFO - ğŸ’» ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 12:03:01,993] A new study created in memory with name: no-name-659b1a60-7948-40fe-a7f3-3de28469e0ab\n",
      "[I 2025-10-27 12:04:04,445] Trial 0 finished with value: 0.000941793060030295 and parameters: {'conv1_ch': 32, 'conv2_ch': 8, 'lstm_hidden': 32, 'dropout': 0.4, 'lr': 0.0020348928640631417}. Best is trial 0 with value: 0.000941793060030295.\n",
      "[I 2025-10-27 12:05:03,280] Trial 1 finished with value: 0.0012896537355396257 and parameters: {'conv1_ch': 32, 'conv2_ch': 24, 'lstm_hidden': 96, 'dropout': 0.5, 'lr': 0.008185642803864832}. Best is trial 0 with value: 0.000941793060030295.\n",
      "[I 2025-10-27 12:05:59,637] Trial 2 finished with value: 0.0009204673577003106 and parameters: {'conv1_ch': 32, 'conv2_ch': 16, 'lstm_hidden': 32, 'dropout': 0.30000000000000004, 'lr': 0.0017884859099584099}. Best is trial 2 with value: 0.0009204673577003106.\n",
      "[I 2025-10-27 12:06:52,997] Trial 3 finished with value: 0.0009158133731046799 and parameters: {'conv1_ch': 16, 'conv2_ch': 16, 'lstm_hidden': 96, 'dropout': 0.30000000000000004, 'lr': 0.002516945050134089}. Best is trial 3 with value: 0.0009158133731046799.\n",
      "[I 2025-10-27 12:07:50,681] Trial 4 finished with value: 0.0010351235338199028 and parameters: {'conv1_ch': 48, 'conv2_ch': 16, 'lstm_hidden': 96, 'dropout': 0.1, 'lr': 0.00524013545657032}. Best is trial 3 with value: 0.0009158133731046799.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# ì‹¤í–‰ ì˜ˆì œ\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_csv = \"C:/ESG_Project1/file/merge_data/train_data.csv\"\n",
    "    test_csv  = \"C:/ESG_Project1/file/merge_data/test_data.csv\"\n",
    "    output_dir = \"C:/ESG_Project1/cnn_lstm/output\"\n",
    "\n",
    "    model, predicted_generation, output_path = run_pipeline_shap_heatmap(\n",
    "        train_csv=train_csv,\n",
    "        test_csv=test_csv,\n",
    "        target_col='í•©ì‚°ë°œì „ëŸ‰(MWh)',\n",
    "        input_steps=168,\n",
    "        output_steps=24,\n",
    "        batch_size=128,\n",
    "        epochs=120,\n",
    "        n_trials=10,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    logger.info(f\"âœ… í†µí•© íŒŒì´í”„ë¼ì¸ + SHAP íˆíŠ¸ë§µ ì™„ë£Œ. ê²°ê³¼ ë””ë ‰í† ë¦¬: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python311]",
   "language": "python",
   "name": "conda-env-python311-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
