{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a462c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m glob\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrapidfuzz\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m process, fuzz\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setup_logger\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from bs4 import BeautifulSoup\n",
    "from rapidfuzz import process, fuzz\n",
    "from util.logger import setup_logger\n",
    "\n",
    "# ===== ë¡œê±° ì„¤ì • =====\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# ğŸ”¹ ê²½ë¡œ ì„¤ì •\n",
    "# ----------------------------------------------------\n",
    "BASE_DIR = \"C:/ESG_Project1/file/\"\n",
    "KMA_DIR = os.path.join(BASE_DIR, \"KMA_data_file/\")\n",
    "SOLAR_DIR = os.path.join(BASE_DIR, \"solar_data_file/\")\n",
    "OUT_CSV = os.path.join(SOLAR_DIR, \"train_data.csv\")\n",
    "\n",
    "CACHE_JSON = os.path.join(BASE_DIR, \"json/mapping_cache.json\")\n",
    "REGION_FIX_JSON = os.path.join(BASE_DIR, \"json/region_fix.json\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# ğŸ”¹ ê³µí†µ ìœ í‹¸\n",
    "# ----------------------------------------------------\n",
    "def sniff_delimiter(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        raw = f.read(2048)\n",
    "    text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "    return \",\" if text.count(\",\") >= text.count(\"\\t\") else \"\\t\"\n",
    "\n",
    "\n",
    "def read_csv_safe(path):\n",
    "    delim = sniff_delimiter(path)\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=\"utf-8\", delimiter=delim)\n",
    "    except UnicodeDecodeError:\n",
    "        logger.warning(f\"{path} UTF-8 ì‹¤íŒ¨ â†’ cp949ë¡œ ì¬ì‹œë„\")\n",
    "        return pd.read_csv(path, encoding=\"cp949\", delimiter=delim)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# ğŸ”¹ ë‚¨ë™ë°œì „ ë°œì „ì†Œ ì§€ì—­ ë§¤í•‘ í¬ë¡¤ëŸ¬\n",
    "# ----------------------------------------------------\n",
    "def crawl_mapping():\n",
    "    URL = \"https://www.koenergy.kr/kosep/hw/fr/ov/ovhw25/main.do?menuCd=FN060202\"\n",
    "    logger.info(f\"í¬ë¡¤ë§ ì‹œì‘: {URL}\")\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    res = requests.get(URL, headers=headers, timeout=10)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    tables = soup.find_all(\"table\", class_=\"table_list2\")\n",
    "    if len(tables) < 2:\n",
    "        logger.error(\"âŒ class='table_list2' í…Œì´ë¸”ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        raise RuntimeError(\"í…Œì´ë¸” ë¶€ì¡±\")\n",
    "\n",
    "    table = tables[1]\n",
    "    mapping = {}\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        tds = [td.get_text(strip=True) for td in tr.find_all(\"td\")]\n",
    "        if len(tds) < 2:\n",
    "            continue\n",
    "        name, region = tds[0], tds[1]\n",
    "        if \"íƒœì–‘ê´‘\" not in name:\n",
    "            continue\n",
    "        name = name.replace(\"ë°œì „ì†Œ\", \"\").replace(\" \", \"\").strip()\n",
    "        mapping[name] = region\n",
    "\n",
    "    os.makedirs(os.path.dirname(CACHE_JSON), exist_ok=True)\n",
    "    with open(CACHE_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(mapping, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    logger.info(f\"{len(mapping)}ê°œ í•­ëª© í¬ë¡¤ë§ ì™„ë£Œ â†’ {CACHE_JSON}\")\n",
    "    return mapping\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# ğŸ”¹ ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# ----------------------------------------------------\n",
    "def normalize_columns(df):\n",
    "    df.columns = df.columns.str.strip()\n",
    "    if \"ë°œì „êµ¬ë¶„\" not in df.columns:\n",
    "        expected = [\"ë°œì „êµ¬ë¶„\", \"í˜¸ê¸°\", \"ì¼ì\"] + [f\"{i}ì‹œ ë°œì „ëŸ‰(MWh)\" for i in range(1, 25)]\n",
    "        df = df.iloc[:, :len(expected)]\n",
    "        df.columns = expected\n",
    "    df[\"ë°œì „êµ¬ë¶„\"] = df[\"ë°œì „êµ¬ë¶„\"].astype(str).str.strip()\n",
    "    df[\"ì¼ì\"] = pd.to_datetime(df[\"ì¼ì\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_hour_cols(df):\n",
    "    return [c for c in df.columns if re.match(r\"^\\s*\\d{1,2}ì‹œ\", c)]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# ğŸ”¹ ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ë°°ì—´ ê¸°ë°˜)\n",
    "# ----------------------------------------------------\n",
    "def process_all_data():\n",
    "    all_data = {}  # ë©”ëª¨ë¦¬ ë‚´ ì €ì¥ì†Œ\n",
    "\n",
    "    # 1ï¸âƒ£ ë°œì „ëŸ‰ ë°ì´í„° ìˆ˜ì§‘\n",
    "    solar_files = glob(os.path.join(SOLAR_DIR, \"*.csv\")) + glob(os.path.join(SOLAR_DIR, \"*.CSV\"))\n",
    "    if not solar_files:\n",
    "        logger.error(\"CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        raise FileNotFoundError(\"ë°œì „ëŸ‰ CSV ì—†ìŒ\")\n",
    "\n",
    "    solar_frames = []\n",
    "    for f in solar_files:\n",
    "        try:\n",
    "            tmp = read_csv_safe(f)\n",
    "            tmp = normalize_columns(tmp)\n",
    "            tmp[\"íŒŒì¼ì¶œì²˜\"] = os.path.basename(f)\n",
    "            solar_frames.append(tmp)\n",
    "            logger.info(f\"ë¶ˆëŸ¬ì˜´: {os.path.basename(f)} ({len(tmp)}í–‰)\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"{os.path.basename(f)} ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    df_solar = pd.concat(solar_frames, ignore_index=True)\n",
    "    df_solar = df_solar.drop_duplicates()\n",
    "\n",
    "    # melt ì²˜ë¦¬\n",
    "    hour_cols = get_hour_cols(df_solar)\n",
    "    df_solar_long = df_solar.melt(\n",
    "        id_vars=[\"ë°œì „êµ¬ë¶„\", \"í˜¸ê¸°\", \"ì¼ì\"],\n",
    "        value_vars=hour_cols,\n",
    "        var_name=\"ì‹œê°„ëŒ€\",\n",
    "        value_name=\"ë°œì „ëŸ‰(MWh)\"\n",
    "    )\n",
    "    df_solar_long[\"ì‹œê°„\"] = df_solar_long[\"ì‹œê°„ëŒ€\"].str.extract(r\"(\\d{1,2})\").astype(int)\n",
    "    df_solar_long[\"ì¼ì‹œ\"] = df_solar_long[\"ì¼ì\"] + pd.to_timedelta(df_solar_long[\"ì‹œê°„\"] - 1, \"h\")\n",
    "    all_data[\"solar\"] = df_solar_long\n",
    "\n",
    "    # 2ï¸âƒ£ ì§€ì—­ ë§¤í•‘ ë¡œë“œ\n",
    "    if os.path.exists(CACHE_JSON):\n",
    "        with open(CACHE_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "            mapping = json.load(f)\n",
    "        logger.info(f\"ê¸°ì¡´ ìºì‹œ ì‚¬ìš© ({len(mapping)}ê±´)\")\n",
    "    else:\n",
    "        mapping = crawl_mapping()\n",
    "    all_data[\"mapping\"] = mapping\n",
    "\n",
    "    # 3ï¸âƒ£ ê¸°ìƒ ë°ì´í„° ìˆ˜ì§‘\n",
    "    weather_files = sorted(glob(os.path.join(KMA_DIR, \"OBS_ASOS_TIM_20*.csv\")))\n",
    "    weather_frames = []\n",
    "    for wf in weather_files:\n",
    "        try:\n",
    "            tmp = read_csv_safe(wf)\n",
    "            tmp[\"ì¼ì‹œ\"] = pd.to_datetime(tmp[\"ì¼ì‹œ\"], errors=\"coerce\")\n",
    "            tmp = tmp[[\"ì§€ì \", \"ì¼ì‹œ\", \"ê¸°ì˜¨(Â°C)\", \"ê°•ìˆ˜ëŸ‰(mm)\", \"ì¼ì¡°(hr)\", \"ì¼ì‚¬(MJ/m2)\"]]\n",
    "            weather_frames.append(tmp)\n",
    "            logger.info(f\"ê¸°ìƒ ë¶ˆëŸ¬ì˜´: {os.path.basename(wf)} ({len(tmp)}í–‰)\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{os.path.basename(wf)} ì‹¤íŒ¨: {e}\")\n",
    "    df_weather = pd.concat(weather_frames, ignore_index=True)\n",
    "    all_data[\"weather\"] = df_weather\n",
    "\n",
    "    # 4ï¸âƒ£ ë°œì „ëŸ‰ + ê¸°ìƒë°ì´í„° ë³‘í•© (ë©”ëª¨ë¦¬ ë‚´)\n",
    "    merged = pd.merge(\n",
    "        all_data[\"solar\"],\n",
    "        all_data[\"weather\"],\n",
    "        on=\"ì¼ì‹œ\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    weather_cols = [\"ê¸°ì˜¨(Â°C)\", \"ê°•ìˆ˜ëŸ‰(mm)\", \"ì¼ì¡°(hr)\", \"ì¼ì‚¬(MJ/m2)\"]\n",
    "    merged[weather_cols] = merged[weather_cols].fillna(0)\n",
    "    all_data[\"merged_final\"] = merged\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# ğŸ”¹ ì‹¤í–‰ë¶€\n",
    "# ----------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"=== ë°ì´í„° í†µí•© ì‹œì‘ ===\")\n",
    "    all_data = process_all_data()\n",
    "\n",
    "    # ë§ˆì§€ë§‰ì—ë§Œ CSV ì €ì¥\n",
    "    all_data[\"merged_final\"].to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    logger.info(f\"âœ… ìµœì¢… ë³‘í•© ì™„ë£Œ â†’ {OUT_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "311.venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
